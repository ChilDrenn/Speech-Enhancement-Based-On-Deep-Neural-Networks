Starting training:
simam:
Namespace(attn='simam', batch_size=4, cut_len=32000, data_dir='/mnt/iusers01/msc-stu/hum-msc-data-sci-2024-2025/t74061zq/erp/DEMAND_16KHz', decay_epoch=12, epochs=60, init_lr=0.0005, log_interval=500, loss_weights=[0.1, 0.9, 0.2, 0.05], save_model_dir='./saved_models_log/saved_models_20250820_simam')
['NVIDIA A100-SXM4-80GB']
====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
TSCNet                                             [1, 1, 321, 201]          --
├─DenseEncoder: 1-1                                [1, 64, 321, 101]         --
│    └─Sequential: 2-1                             [1, 64, 321, 201]         --
│    │    └─Conv2d: 3-1                            [1, 64, 321, 201]         256
│    │    └─InstanceNorm2d: 3-2                    [1, 64, 321, 201]         128
│    │    └─PReLU: 3-3                             [1, 64, 321, 201]         64
│    └─Simam_module: 2-2                           [1, 64, 321, 201]         --
│    │    └─Sigmoid: 3-4                           [1, 64, 321, 201]         --
│    └─DilatedDenseNet: 2-3                        [1, 64, 321, 201]         --
│    │    └─ConstantPad2d: 3-5                     [1, 64, 322, 203]         --
│    │    └─Conv2d: 3-6                            [1, 64, 321, 201]         24,640
│    │    └─InstanceNorm2d: 3-7                    [1, 64, 321, 201]         128
│    │    └─PReLU: 3-8                             [1, 64, 321, 201]         64
│    │    └─ConstantPad2d: 3-9                     [1, 128, 323, 203]        --
│    │    └─Conv2d: 3-10                           [1, 64, 321, 201]         49,216
│    │    └─InstanceNorm2d: 3-11                   [1, 64, 321, 201]         128
│    │    └─PReLU: 3-12                            [1, 64, 321, 201]         64
│    │    └─ConstantPad2d: 3-13                    [1, 192, 325, 203]        --
│    │    └─Conv2d: 3-14                           [1, 64, 321, 201]         73,792
│    │    └─InstanceNorm2d: 3-15                   [1, 64, 321, 201]         128
│    │    └─PReLU: 3-16                            [1, 64, 321, 201]         64
│    │    └─ConstantPad2d: 3-17                    [1, 256, 329, 203]        --
│    │    └─Conv2d: 3-18                           [1, 64, 321, 201]         98,368
│    │    └─InstanceNorm2d: 3-19                   [1, 64, 321, 201]         128
│    │    └─PReLU: 3-20                            [1, 64, 321, 201]         64
│    └─Sequential: 2-4                             [1, 64, 321, 101]         --
│    │    └─Conv2d: 3-21                           [1, 64, 321, 101]         12,352
│    │    └─InstanceNorm2d: 3-22                   [1, 64, 321, 101]         128
│    │    └─PReLU: 3-23                            [1, 64, 321, 101]         64
├─TSCB: 1-2                                        [1, 64, 321, 101]         --
│    └─ConformerBlock: 2-5                         [101, 321, 64]            --
│    │    └─Scale: 3-24                            [101, 321, 64]            33,216
│    │    └─PreNorm: 3-25                          [101, 321, 64]            32,976
│    │    └─ConformerConvModule: 3-26              [101, 321, 64]            29,376
│    │    └─Scale: 3-27                            [101, 321, 64]            33,216
│    │    └─LayerNorm: 3-28                        [101, 321, 64]            128
│    └─ConformerBlock: 2-6                         [321, 101, 64]            --
│    │    └─Scale: 3-29                            [321, 101, 64]            33,216
│    │    └─PreNorm: 3-30                          [321, 101, 64]            32,976
│    │    └─ConformerConvModule: 3-31              [321, 101, 64]            29,376
│    │    └─Scale: 3-32                            [321, 101, 64]            33,216
│    │    └─LayerNorm: 3-33                        [321, 101, 64]            128
├─TSCB: 1-3                                        [1, 64, 321, 101]         --
│    └─ConformerBlock: 2-7                         [101, 321, 64]            --
│    │    └─Scale: 3-34                            [101, 321, 64]            33,216
│    │    └─PreNorm: 3-35                          [101, 321, 64]            32,976
│    │    └─ConformerConvModule: 3-36              [101, 321, 64]            29,376
│    │    └─Scale: 3-37                            [101, 321, 64]            33,216
│    │    └─LayerNorm: 3-38                        [101, 321, 64]            128
│    └─ConformerBlock: 2-8                         [321, 101, 64]            --
│    │    └─Scale: 3-39                            [321, 101, 64]            33,216
│    │    └─PreNorm: 3-40                          [321, 101, 64]            32,976
│    │    └─ConformerConvModule: 3-41              [321, 101, 64]            29,376
│    │    └─Scale: 3-42                            [321, 101, 64]            33,216
│    │    └─LayerNorm: 3-43                        [321, 101, 64]            128
├─TSCB: 1-4                                        [1, 64, 321, 101]         --
│    └─ConformerBlock: 2-9                         [101, 321, 64]            --
│    │    └─Scale: 3-44                            [101, 321, 64]            33,216
│    │    └─PreNorm: 3-45                          [101, 321, 64]            32,976
│    │    └─ConformerConvModule: 3-46              [101, 321, 64]            29,376
│    │    └─Scale: 3-47                            [101, 321, 64]            33,216
│    │    └─LayerNorm: 3-48                        [101, 321, 64]            128
│    └─ConformerBlock: 2-10                        [321, 101, 64]            --
│    │    └─Scale: 3-49                            [321, 101, 64]            33,216
│    │    └─PreNorm: 3-50                          [321, 101, 64]            32,976
│    │    └─ConformerConvModule: 3-51              [321, 101, 64]            29,376
│    │    └─Scale: 3-52                            [321, 101, 64]            33,216
│    │    └─LayerNorm: 3-53                        [321, 101, 64]            128
├─TSCB: 1-5                                        [1, 64, 321, 101]         --
│    └─ConformerBlock: 2-11                        [101, 321, 64]            --
│    │    └─Scale: 3-54                            [101, 321, 64]            33,216
│    │    └─PreNorm: 3-55                          [101, 321, 64]            32,976
│    │    └─ConformerConvModule: 3-56              [101, 321, 64]            29,376
│    │    └─Scale: 3-57                            [101, 321, 64]            33,216
│    │    └─LayerNorm: 3-58                        [101, 321, 64]            128
│    └─ConformerBlock: 2-12                        [321, 101, 64]            --
│    │    └─Scale: 3-59                            [321, 101, 64]            33,216
│    │    └─PreNorm: 3-60                          [321, 101, 64]            32,976
│    │    └─ConformerConvModule: 3-61              [321, 101, 64]            29,376
│    │    └─Scale: 3-62                            [321, 101, 64]            33,216
│    │    └─LayerNorm: 3-63                        [321, 101, 64]            128
├─MaskDecoder: 1-6                                 [1, 1, 321, 201]          --
│    └─DilatedDenseNet: 2-13                       [1, 64, 321, 101]         --
│    │    └─ConstantPad2d: 3-64                    [1, 64, 322, 103]         --
│    │    └─Conv2d: 3-65                           [1, 64, 321, 101]         24,640
│    │    └─InstanceNorm2d: 3-66                   [1, 64, 321, 101]         128
│    │    └─PReLU: 3-67                            [1, 64, 321, 101]         64
│    │    └─ConstantPad2d: 3-68                    [1, 128, 323, 103]        --
│    │    └─Conv2d: 3-69                           [1, 64, 321, 101]         49,216
│    │    └─InstanceNorm2d: 3-70                   [1, 64, 321, 101]         128
│    │    └─PReLU: 3-71                            [1, 64, 321, 101]         64
│    │    └─ConstantPad2d: 3-72                    [1, 192, 325, 103]        --
│    │    └─Conv2d: 3-73                           [1, 64, 321, 101]         73,792
│    │    └─InstanceNorm2d: 3-74                   [1, 64, 321, 101]         128
│    │    └─PReLU: 3-75                            [1, 64, 321, 101]         64
│    │    └─ConstantPad2d: 3-76                    [1, 256, 329, 103]        --
│    │    └─Conv2d: 3-77                           [1, 64, 321, 101]         98,368
│    │    └─InstanceNorm2d: 3-78                   [1, 64, 321, 101]         128
│    │    └─PReLU: 3-79                            [1, 64, 321, 101]         64
│    └─Simam_module: 2-14                          [1, 64, 321, 101]         --
│    │    └─Sigmoid: 3-80                          [1, 64, 321, 101]         --
│    └─SPConvTranspose2d: 2-15                     [1, 64, 321, 202]         --
│    │    └─ConstantPad2d: 3-81                    [1, 64, 321, 103]         --
│    │    └─Conv2d: 3-82                           [1, 128, 321, 101]        24,704
│    └─Conv2d: 2-16                                [1, 1, 321, 201]          129
│    └─InstanceNorm2d: 2-17                        [1, 1, 321, 201]          2
│    └─PReLU: 2-18                                 [1, 1, 321, 201]          1
│    └─Conv2d: 2-19                                [1, 1, 321, 201]          2
│    └─PReLU: 2-20                                 [1, 201, 321]             201
├─ComplexDecoder: 1-7                              [1, 2, 321, 201]          --
│    └─DilatedDenseNet: 2-21                       [1, 64, 321, 101]         --
│    │    └─ConstantPad2d: 3-83                    [1, 64, 322, 103]         --
│    │    └─Conv2d: 3-84                           [1, 64, 321, 101]         24,640
│    │    └─InstanceNorm2d: 3-85                   [1, 64, 321, 101]         128
│    │    └─PReLU: 3-86                            [1, 64, 321, 101]         64
│    │    └─ConstantPad2d: 3-87                    [1, 128, 323, 103]        --
│    │    └─Conv2d: 3-88                           [1, 64, 321, 101]         49,216
│    │    └─InstanceNorm2d: 3-89                   [1, 64, 321, 101]         128
│    │    └─PReLU: 3-90                            [1, 64, 321, 101]         64
│    │    └─ConstantPad2d: 3-91                    [1, 192, 325, 103]        --
│    │    └─Conv2d: 3-92                           [1, 64, 321, 101]         73,792
│    │    └─InstanceNorm2d: 3-93                   [1, 64, 321, 101]         128
│    │    └─PReLU: 3-94                            [1, 64, 321, 101]         64
│    │    └─ConstantPad2d: 3-95                    [1, 256, 329, 103]        --
│    │    └─Conv2d: 3-96                           [1, 64, 321, 101]         98,368
│    │    └─InstanceNorm2d: 3-97                   [1, 64, 321, 101]         128
│    │    └─PReLU: 3-98                            [1, 64, 321, 101]         64
│    └─Simam_module: 2-22                          [1, 64, 321, 101]         --
│    │    └─Sigmoid: 3-99                          [1, 64, 321, 101]         --
│    └─SPConvTranspose2d: 2-23                     [1, 64, 321, 202]         --
│    │    └─ConstantPad2d: 3-100                   [1, 64, 321, 103]         --
│    │    └─Conv2d: 3-101                          [1, 128, 321, 101]        24,704
│    └─InstanceNorm2d: 2-24                        [1, 64, 321, 202]         128
│    └─PReLU: 2-25                                 [1, 64, 321, 202]         64
│    └─Conv2d: 2-26                                [1, 2, 321, 201]          258
====================================================================================================
Total params: 1,834,833
Trainable params: 1,834,833
Non-trainable params: 0
Total mult-adds (G): 41.56
====================================================================================================
Input size (MB): 0.52
Forward/backward pass size (MB): 4856.40
Params size (MB): 7.34
Estimated Total Size (MB): 4864.25
====================================================================================================
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
Discriminator                            [1, 1]                    --
├─Sequential: 1-1                        [1, 1]                    --
│    └─Conv2d: 2-1                       [1, 16, 100, 160]         512
│    └─InstanceNorm2d: 2-2               [1, 16, 100, 160]         32
│    └─PReLU: 2-3                        [1, 16, 100, 160]         16
│    └─Conv2d: 2-4                       [1, 32, 50, 80]           8,192
│    └─InstanceNorm2d: 2-5               [1, 32, 50, 80]           64
│    └─PReLU: 2-6                        [1, 32, 50, 80]           32
│    └─Conv2d: 2-7                       [1, 64, 25, 40]           32,768
│    └─InstanceNorm2d: 2-8               [1, 64, 25, 40]           128
│    └─PReLU: 2-9                        [1, 64, 25, 40]           64
│    └─Conv2d: 2-10                      [1, 128, 12, 20]          131,072
│    └─InstanceNorm2d: 2-11              [1, 128, 12, 20]          256
│    └─PReLU: 2-12                       [1, 128, 12, 20]          128
│    └─AdaptiveMaxPool2d: 2-13           [1, 128, 1, 1]            --
│    └─Flatten: 2-14                     [1, 128]                  --
│    └─Linear: 2-15                      [1, 64]                   8,256
│    └─Dropout: 2-16                     [1, 64]                   --
│    └─PReLU: 2-17                       [1, 64]                   64
│    └─Linear: 2-18                      [1, 1]                    65
│    └─LearnableSigmoid: 2-19            [1, 1]                    1
==========================================================================================
Total params: 181,650
Trainable params: 181,650
Non-trainable params: 0
Total mult-adds (M): 19.67
==========================================================================================
Input size (MB): 0.52
Forward/backward pass size (MB): 11.49
Params size (MB): 0.73
Estimated Total Size (MB): 12.73
==========================================================================================
/mnt/iusers01/msc-stu/hum-msc-data-sci-2024-2025/t74061zq/erp/CMGAN_src/data/dataloader.py:52: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend("sox_io")         # in linux
INFO:root:Epoch 0, Step 500, loss: 0.153137668967247, disc_loss: 0.02719796448945999
INFO:root:Epoch 0, Step 1000, loss: 0.10541414469480515, disc_loss: 0.009023145772516727
INFO:root:Epoch 0, Step 1500, loss: 0.1525273621082306, disc_loss: 0.011180637404322624
INFO:root:Epoch 0, Step 2000, loss: 0.14024274051189423, disc_loss: 0.02451443113386631
INFO:root:Epoch 0, Step 2500, loss: 0.154636949300766, disc_loss: 0.006133463233709335
INFO:root:Generator loss: 0.08417400524237202, Discriminator loss: 0.008143231612477606
INFO:root:
=== Epoch 0 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 2.8108,SSNR: 8.0865,CSIG: 4.3495,CBAK: 3.4528,COVL: 3.6409,STOI: 0.9375

Epoch 1/60 finished in 23.79 minutes
INFO:root:Epoch 1, Step 500, loss: 0.15037281811237335, disc_loss: 0.003547373926267028
INFO:root:Epoch 1, Step 1000, loss: 0.10762877017259598, disc_loss: 0.0011264935601502657
INFO:root:Epoch 1, Step 1500, loss: 0.10073456168174744, disc_loss: 0.005309608299285173
INFO:root:Epoch 1, Step 2000, loss: 0.1102796271443367, disc_loss: 0.005548791494220495
INFO:root:Epoch 1, Step 2500, loss: 0.0740039274096489, disc_loss: 0.005216135177761316
INFO:root:Generator loss: 0.0795252200179887, Discriminator loss: 0.006676406470874496
INFO:root:
=== Epoch 1 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 2.8696,SSNR: 8.9116,CSIG: 4.3417,CBAK: 3.5342,COVL: 3.6647,STOI: 0.9424

Epoch 2/60 finished in 23.91 minutes
INFO:root:Epoch 2, Step 500, loss: 0.09377936273813248, disc_loss: 0.004370648879557848
INFO:root:Epoch 2, Step 1000, loss: 0.12347638607025146, disc_loss: 0.0036875864025205374
INFO:root:Epoch 2, Step 1500, loss: 0.11846926808357239, disc_loss: 0.0016996168997138739
INFO:root:Epoch 2, Step 2000, loss: 0.14755405485630035, disc_loss: 0.017276862636208534
INFO:root:Epoch 2, Step 2500, loss: 0.08247076719999313, disc_loss: 0.0047550322487950325
INFO:root:Generator loss: 0.07359325927365752, Discriminator loss: 0.005601211101182927
INFO:root:
=== Epoch 2 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 2.9715,SSNR: 9.3651,CSIG: 4.4338,CBAK: 3.6151,COVL: 3.7665,STOI: 0.9458

Epoch 3/60 finished in 23.56 minutes
INFO:root:Epoch 3, Step 500, loss: 0.14748717844486237, disc_loss: 0.009084932506084442
INFO:root:Epoch 3, Step 1000, loss: 0.08973561972379684, disc_loss: 0.012448837980628014
INFO:root:Epoch 3, Step 1500, loss: 0.10425593703985214, disc_loss: 0.01060392428189516
INFO:root:Epoch 3, Step 2000, loss: 0.11378718167543411, disc_loss: 0.006449379958212376
INFO:root:Epoch 3, Step 2500, loss: 0.1069401353597641, disc_loss: 0.009027662687003613
INFO:root:Generator loss: 0.08309015857724888, Discriminator loss: 0.0068177134225973456
INFO:root:
=== Epoch 3 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 2.7903,SSNR: 8.7464,CSIG: 4.2666,CBAK: 3.4875,COVL: 3.5849,STOI: 0.9467

Epoch 4/60 finished in 23.90 minutes
INFO:root:Epoch 4, Step 500, loss: 0.08509695529937744, disc_loss: 0.0026398925110697746
INFO:root:Epoch 4, Step 1000, loss: 0.12914489209651947, disc_loss: 0.0023522456176579
INFO:root:Epoch 4, Step 1500, loss: 0.07108286023139954, disc_loss: 0.019542381167411804
INFO:root:Epoch 4, Step 2000, loss: 0.07797017693519592, disc_loss: 0.004530434962362051
INFO:root:Epoch 4, Step 2500, loss: 0.09625270217657089, disc_loss: 0.0006239537033252418
INFO:root:Generator loss: 0.07154984159160008, Discriminator loss: 0.005870550918930834
INFO:root:
=== Epoch 4 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.0726,SSNR: 9.1379,CSIG: 4.4928,CBAK: 3.6482,COVL: 3.8558,STOI: 0.9484

Epoch 5/60 finished in 23.91 minutes
INFO:root:Epoch 5, Step 500, loss: 0.07005917280912399, disc_loss: 0.0008262910996563733
INFO:root:Epoch 5, Step 1000, loss: 0.09204762428998947, disc_loss: 0.008514481596648693
INFO:root:Epoch 5, Step 1500, loss: 0.06725919246673584, disc_loss: 0.0017110834596678615
INFO:root:Epoch 5, Step 2000, loss: 0.06419239938259125, disc_loss: 0.0011708445381373167
INFO:root:Epoch 5, Step 2500, loss: 0.05965859442949295, disc_loss: 0.04487781599164009
INFO:root:Generator loss: 0.07139412212429695, Discriminator loss: 0.008165493628430606
INFO:root:
=== Epoch 5 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.0182,SSNR: 9.3632,CSIG: 4.4216,CBAK: 3.6358,COVL: 3.7885,STOI: 0.9497

Epoch 6/60 finished in 23.58 minutes
INFO:root:Epoch 6, Step 500, loss: 0.10389827191829681, disc_loss: 0.00471031479537487
INFO:root:Epoch 6, Step 1000, loss: 0.12605199217796326, disc_loss: 0.005428662523627281
INFO:root:Epoch 6, Step 1500, loss: 0.12837380170822144, disc_loss: 0.00163892877753824
INFO:root:Epoch 6, Step 2000, loss: 0.08899659663438797, disc_loss: 0.006350682117044926
INFO:root:Epoch 6, Step 2500, loss: 0.05819671228528023, disc_loss: 0.0015263798413798213
INFO:root:Generator loss: 0.06771711923760697, Discriminator loss: 0.005938552718273889
INFO:root:
=== Epoch 6 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.1622,SSNR: 9.8121,CSIG: 4.4774,CBAK: 3.7359,COVL: 3.8957,STOI: 0.9491

Epoch 7/60 finished in 23.50 minutes
INFO:root:Epoch 7, Step 500, loss: 0.09487078338861465, disc_loss: 0.0009425722528249025
INFO:root:Epoch 7, Step 1000, loss: 0.07277954369783401, disc_loss: 0.04861956089735031
INFO:root:Epoch 7, Step 1500, loss: 0.07088178396224976, disc_loss: 0.003805315587669611
INFO:root:Epoch 7, Step 2000, loss: 0.07814931124448776, disc_loss: 0.0037715767975896597
INFO:root:Epoch 7, Step 2500, loss: 0.07049170136451721, disc_loss: 0.012244819663465023
INFO:root:Generator loss: 0.0646319226407021, Discriminator loss: 0.004592692213600899
INFO:root:
=== Epoch 7 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.1840,SSNR: 10.1615,CSIG: 4.5179,CBAK: 3.7683,COVL: 3.9276,STOI: 0.9513

Epoch 8/60 finished in 23.51 minutes
INFO:root:Epoch 8, Step 500, loss: 0.0845964103937149, disc_loss: 0.002596916165202856
INFO:root:Epoch 8, Step 1000, loss: 0.11076635122299194, disc_loss: 0.0006328571471385658
INFO:root:Epoch 8, Step 1500, loss: 0.1110907718539238, disc_loss: 0.004017038270831108
INFO:root:Epoch 8, Step 2000, loss: 0.07421637326478958, disc_loss: 0.0005662305047735572
INFO:root:Epoch 8, Step 2500, loss: 0.11020256578922272, disc_loss: 0.0017276316648349166
INFO:root:Generator loss: 0.06608890572074548, Discriminator loss: 0.007473327014381936
INFO:root:
=== Epoch 8 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.1401,SSNR: 9.6533,CSIG: 4.4825,CBAK: 3.7160,COVL: 3.8835,STOI: 0.9529

Epoch 9/60 finished in 23.51 minutes
INFO:root:Epoch 9, Step 500, loss: 0.06485152244567871, disc_loss: 0.0012213819427415729
INFO:root:Epoch 9, Step 1000, loss: 0.11713393777608871, disc_loss: 0.006906797643750906
INFO:root:Epoch 9, Step 1500, loss: 0.10058027505874634, disc_loss: 0.004439355339854956
INFO:root:Epoch 9, Step 2000, loss: 0.071450375020504, disc_loss: 0.005688406992703676
INFO:root:Epoch 9, Step 2500, loss: 0.09737426787614822, disc_loss: 0.0008128698682412505
INFO:root:Generator loss: 0.06256116009814647, Discriminator loss: 0.007127322072546943
INFO:root:
=== Epoch 9 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.2328,SSNR: 10.0837,CSIG: 4.5460,CBAK: 3.7893,COVL: 3.9760,STOI: 0.9508

Epoch 10/60 finished in 23.53 minutes
INFO:root:Epoch 10, Step 500, loss: 0.08137111365795135, disc_loss: 0.0027048070915043354
INFO:root:Epoch 10, Step 1000, loss: 0.11570709943771362, disc_loss: 0.0015151541447266936
INFO:root:Epoch 10, Step 1500, loss: 0.07357800006866455, disc_loss: 0.003385028103366494
INFO:root:Epoch 10, Step 2000, loss: 0.10995248705148697, disc_loss: 0.0013596368953585625
INFO:root:Epoch 10, Step 2500, loss: 0.0864047184586525, disc_loss: 0.0010632369667291641
INFO:root:Generator loss: 0.06530147217648122, Discriminator loss: 0.005413516073142833
INFO:root:
=== Epoch 10 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.2324,SSNR: 8.8456,CSIG: 4.5017,CBAK: 3.7111,COVL: 3.9478,STOI: 0.9528

Epoch 11/60 finished in 23.78 minutes
INFO:root:Epoch 11, Step 500, loss: 0.05737999081611633, disc_loss: 0.0030028517358005047
INFO:root:Epoch 11, Step 1000, loss: 0.07058563828468323, disc_loss: 0.0014142642030492425
INFO:root:Epoch 11, Step 1500, loss: 0.09676865488290787, disc_loss: 0.002920198254287243
INFO:root:Epoch 11, Step 2000, loss: 0.04807283729314804, disc_loss: 0.003035247093066573
INFO:root:Epoch 11, Step 2500, loss: 0.10987898707389832, disc_loss: 0.002334476448595524
INFO:root:Generator loss: 0.06319207743316599, Discriminator loss: 0.00513162793493711
INFO:root:
=== Epoch 11 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.2016,SSNR: 10.1087,CSIG: 4.5407,CBAK: 3.7759,COVL: 3.9515,STOI: 0.9528

Epoch 12/60 finished in 23.80 minutes
INFO:root:Epoch 12, Step 500, loss: 0.08013089001178741, disc_loss: 0.0007307012565433979
INFO:root:Epoch 12, Step 1000, loss: 0.06642327457666397, disc_loss: 0.0025277489330619574
INFO:root:Epoch 12, Step 1500, loss: 0.08368808031082153, disc_loss: 0.0029095481149852276
INFO:root:Epoch 12, Step 2000, loss: 0.09886854141950607, disc_loss: 0.0016524356324225664
INFO:root:Epoch 12, Step 2500, loss: 0.08628157526254654, disc_loss: 0.0005702534690499306
INFO:root:Generator loss: 0.06131730933935897, Discriminator loss: 0.005249682635869108
INFO:root:
=== Epoch 12 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.2142,SSNR: 10.2694,CSIG: 4.5438,CBAK: 3.7921,COVL: 3.9608,STOI: 0.9541

Epoch 13/60 finished in 23.72 minutes
/mnt/iusers01/msc-stu/hum-msc-data-sci-2024-2025/t74061zq/.conda/envs/CMGAN/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.
  warnings.warn(
INFO:root:Epoch 13, Step 500, loss: 0.069803886115551, disc_loss: 0.0008952523348852992
INFO:root:Epoch 13, Step 1000, loss: 0.08263243734836578, disc_loss: 0.0016847385559231043
INFO:root:Epoch 13, Step 1500, loss: 0.07552050054073334, disc_loss: 0.0016156090423464775
INFO:root:Epoch 13, Step 2000, loss: 0.09022372215986252, disc_loss: 0.003181382082402706
INFO:root:Epoch 13, Step 2500, loss: 0.07719039171934128, disc_loss: 0.001260067569091916
INFO:root:Generator loss: 0.06079569500554534, Discriminator loss: 0.005454734437610136
INFO:root:
=== Epoch 13 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.3027,SSNR: 10.0658,CSIG: 4.5690,CBAK: 3.8222,COVL: 4.0250,STOI: 0.9526

Epoch 14/60 finished in 23.79 minutes
INFO:root:Epoch 14, Step 500, loss: 0.07954806089401245, disc_loss: 0.0024360367096960545
INFO:root:Epoch 14, Step 1000, loss: 0.08014615625143051, disc_loss: 0.002430911408737302
INFO:root:Epoch 14, Step 1500, loss: 0.11637842655181885, disc_loss: 0.0030400948598980904
INFO:root:Epoch 14, Step 2000, loss: 0.07625801116228104, disc_loss: 0.0010343472240492702
INFO:root:Epoch 14, Step 2500, loss: 0.09889192879199982, disc_loss: 0.0021423171274363995
INFO:root:Generator loss: 0.06279967768678388, Discriminator loss: 0.005254669392140112
INFO:root:
=== Epoch 14 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.2350,SSNR: 9.9038,CSIG: 4.5037,CBAK: 3.7786,COVL: 3.9506,STOI: 0.9536

Epoch 15/60 finished in 23.59 minutes
INFO:root:Epoch 15, Step 500, loss: 0.11630723625421524, disc_loss: 0.001069660298526287
INFO:root:Epoch 15, Step 1000, loss: 0.10138516873121262, disc_loss: 0.0002970487985294312
INFO:root:Epoch 15, Step 1500, loss: 0.059617333114147186, disc_loss: 0.004424992948770523
INFO:root:Epoch 15, Step 2000, loss: 0.04810336232185364, disc_loss: 0.00232325354591012
INFO:root:Epoch 15, Step 2500, loss: 0.07701905816793442, disc_loss: 0.0009975391440093517
INFO:root:Generator loss: 0.05989892752750695, Discriminator loss: 0.005115336740504751
INFO:root:
=== Epoch 15 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.2414,SSNR: 10.4220,CSIG: 4.5299,CBAK: 3.8148,COVL: 3.9672,STOI: 0.9544

Epoch 16/60 finished in 23.64 minutes
INFO:root:Epoch 16, Step 500, loss: 0.07035335898399353, disc_loss: 0.0035853951703757048
INFO:root:Epoch 16, Step 1000, loss: 0.05526953190565109, disc_loss: 0.0019499275367707014
INFO:root:Epoch 16, Step 1500, loss: 0.1550813466310501, disc_loss: 0.0011381534859538078
INFO:root:Epoch 16, Step 2000, loss: 0.07551377266645432, disc_loss: 0.00318160024471581
INFO:root:Epoch 16, Step 2500, loss: 0.09430558979511261, disc_loss: 0.0039293342269957066
INFO:root:Generator loss: 0.05816276436581195, Discriminator loss: 0.003866825717108158
INFO:root:
=== Epoch 16 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.2902,SSNR: 10.4759,CSIG: 4.5400,CBAK: 3.8414,COVL: 3.9984,STOI: 0.9545

Epoch 17/60 finished in 23.61 minutes
INFO:root:Epoch 17, Step 500, loss: 0.050272371619939804, disc_loss: 0.003072362393140793
INFO:root:Epoch 17, Step 1000, loss: 0.11000929772853851, disc_loss: 0.004418112803250551
INFO:root:Epoch 17, Step 1500, loss: 0.09673088788986206, disc_loss: 0.0017150712665170431
INFO:root:Epoch 17, Step 2000, loss: 0.0743364542722702, disc_loss: 0.0043844967149198055
INFO:root:Epoch 17, Step 2500, loss: 0.10476520657539368, disc_loss: 0.0016213981434702873
INFO:root:Generator loss: 0.06286150228810831, Discriminator loss: 0.007032270642703578
INFO:root:
=== Epoch 17 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.2864,SSNR: 9.0332,CSIG: 4.5749,CBAK: 3.7478,COVL: 4.0235,STOI: 0.9538

Epoch 18/60 finished in 23.42 minutes
INFO:root:Epoch 18, Step 500, loss: 0.09248733520507812, disc_loss: 0.0022787025664001703
INFO:root:Epoch 18, Step 1000, loss: 0.06701703369617462, disc_loss: 0.0012254812754690647
INFO:root:Epoch 18, Step 1500, loss: 0.05689086392521858, disc_loss: 0.0009844889864325523
INFO:root:Epoch 18, Step 2000, loss: 0.09188718348741531, disc_loss: 0.0007853925344534218
INFO:root:Epoch 18, Step 2500, loss: 0.07641243934631348, disc_loss: 0.001150578842498362
INFO:root:Generator loss: 0.060871362161723154, Discriminator loss: 0.005383090256057506
INFO:root:
=== Epoch 18 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.2141,SSNR: 10.2608,CSIG: 4.5599,CBAK: 3.7930,COVL: 3.9696,STOI: 0.9550

Epoch 19/60 finished in 23.35 minutes
INFO:root:Epoch 19, Step 500, loss: 0.059186290949583054, disc_loss: 0.0029306390788406134
INFO:root:Epoch 19, Step 1000, loss: 0.07431329041719437, disc_loss: 0.00036086313775740564
INFO:root:Epoch 19, Step 1500, loss: 0.08245966583490372, disc_loss: 0.0015893845120444894
INFO:root:Epoch 19, Step 2000, loss: 0.055631402879953384, disc_loss: 0.01997838169336319
INFO:root:Epoch 19, Step 2500, loss: 0.08398111909627914, disc_loss: 0.0012889290228486061
INFO:root:Generator loss: 0.05825717298729906, Discriminator loss: 0.005839517925390115
INFO:root:
=== Epoch 19 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.3298,SSNR: 10.2853,CSIG: 4.5872,CBAK: 3.8487,COVL: 4.0552,STOI: 0.9530

Epoch 20/60 finished in 23.47 minutes
INFO:root:Epoch 20, Step 500, loss: 0.06450144946575165, disc_loss: 0.0016556581249460578
INFO:root:Epoch 20, Step 1000, loss: 0.13371655344963074, disc_loss: 0.0034540167544037104
INFO:root:Epoch 20, Step 1500, loss: 0.07338795065879822, disc_loss: 0.0019523499067872763
INFO:root:Epoch 20, Step 2000, loss: 0.0605168417096138, disc_loss: 0.0022706796880811453
INFO:root:Epoch 20, Step 2500, loss: 0.09154070168733597, disc_loss: 0.0010507699334993958
INFO:root:Generator loss: 0.05916065522876469, Discriminator loss: 0.004835268683537719
INFO:root:
=== Epoch 20 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.2822,SSNR: 10.3545,CSIG: 4.5797,CBAK: 3.8306,COVL: 4.0210,STOI: 0.9546

Epoch 21/60 finished in 23.46 minutes
INFO:root:Epoch 21, Step 500, loss: 0.08202587068080902, disc_loss: 0.0006583020440302789
INFO:root:Epoch 21, Step 1000, loss: 0.08594296872615814, disc_loss: 0.03480733931064606
INFO:root:Epoch 21, Step 1500, loss: 0.10816352814435959, disc_loss: 0.0017567226896062493
INFO:root:Epoch 21, Step 2000, loss: 0.09035394340753555, disc_loss: 0.0013414670247584581
INFO:root:Epoch 21, Step 2500, loss: 0.07968045771121979, disc_loss: 0.0017650948138907552
INFO:root:Generator loss: 0.058932513107084535, Discriminator loss: 0.004594884733502749
INFO:root:
=== Epoch 21 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.3207,SSNR: 10.0821,CSIG: 4.5428,CBAK: 3.8319,COVL: 4.0221,STOI: 0.9544

Epoch 22/60 finished in 23.41 minutes
INFO:root:Epoch 22, Step 500, loss: 0.07514190673828125, disc_loss: 0.0013073672307655215
INFO:root:Epoch 22, Step 1000, loss: 0.06540347635746002, disc_loss: 0.00041200389387086034
INFO:root:Epoch 22, Step 1500, loss: 0.106397844851017, disc_loss: 0.0016688635805621743
INFO:root:Epoch 22, Step 2000, loss: 0.054200902581214905, disc_loss: 0.0013605317799374461
INFO:root:Epoch 22, Step 2500, loss: 0.07937447726726532, disc_loss: 0.0026736946310847998
INFO:root:Generator loss: 0.05964374274594113, Discriminator loss: 0.0038289843051970114
INFO:root:
=== Epoch 22 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.2701,SSNR: 10.3768,CSIG: 4.5482,CBAK: 3.8262,COVL: 3.9973,STOI: 0.9542

Epoch 23/60 finished in 23.57 minutes
INFO:root:Epoch 23, Step 500, loss: 0.057006560266017914, disc_loss: 0.00011461582471383736
INFO:root:Epoch 23, Step 1000, loss: 0.07121671736240387, disc_loss: 0.0003564227663446218
INFO:root:Epoch 23, Step 1500, loss: 0.0781729519367218, disc_loss: 0.000869237759616226
INFO:root:Epoch 23, Step 2000, loss: 0.07501358538866043, disc_loss: 0.0012748244917020202
INFO:root:Epoch 23, Step 2500, loss: 0.09777812659740448, disc_loss: 0.0016222799895331264
INFO:root:Generator loss: 0.06088843790687693, Discriminator loss: 0.005712718680150678
INFO:root:
=== Epoch 23 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.3097,SSNR: 10.0777,CSIG: 4.5941,CBAK: 3.8268,COVL: 4.0486,STOI: 0.9548

Epoch 24/60 finished in 23.73 minutes
INFO:root:Epoch 24, Step 500, loss: 0.0821593627333641, disc_loss: 0.00375898159109056
INFO:root:Epoch 24, Step 1000, loss: 0.051269859075546265, disc_loss: 0.0009174910373985767
INFO:root:Epoch 24, Step 1500, loss: 0.08947880566120148, disc_loss: 0.00031274883076548576
INFO:root:Epoch 24, Step 2000, loss: 0.0729108601808548, disc_loss: 0.0007718867855146527
INFO:root:Epoch 24, Step 2500, loss: 0.040909189730882645, disc_loss: 0.001964014722034335
INFO:root:Generator loss: 0.060120508689614174, Discriminator loss: 0.006355402847192927
INFO:root:
=== Epoch 24 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.2744,SSNR: 10.1243,CSIG: 4.5501,CBAK: 3.8104,COVL: 4.0020,STOI: 0.9553

Epoch 25/60 finished in 23.72 minutes
INFO:root:Epoch 25, Step 500, loss: 0.08295994251966476, disc_loss: 0.0027488002087920904
INFO:root:Epoch 25, Step 1000, loss: 0.11319012194871902, disc_loss: 0.0029707597568631172
INFO:root:Epoch 25, Step 1500, loss: 0.08873690664768219, disc_loss: 0.0007094519096426666
INFO:root:Epoch 25, Step 2000, loss: 0.05973612517118454, disc_loss: 0.000737938389647752
INFO:root:Epoch 25, Step 2500, loss: 0.0724504217505455, disc_loss: 0.0008104232256300747
INFO:root:Generator loss: 0.05990142959341841, Discriminator loss: 0.004792618048421132
INFO:root:
=== Epoch 25 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.3072,SSNR: 10.2239,CSIG: 4.5646,CBAK: 3.8331,COVL: 4.0285,STOI: 0.9549

Epoch 26/60 finished in 23.71 minutes
INFO:root:Epoch 26, Step 500, loss: 0.07294383645057678, disc_loss: 0.0015335815260186791
INFO:root:Epoch 26, Step 1000, loss: 0.07307206094264984, disc_loss: 0.0007882522186264396
INFO:root:Epoch 26, Step 1500, loss: 0.07078371942043304, disc_loss: 0.00046391255455091596
INFO:root:Epoch 26, Step 2000, loss: 0.08992370218038559, disc_loss: 0.002617749385535717
INFO:root:Epoch 26, Step 2500, loss: 0.05453931540250778, disc_loss: 0.0015197077300399542
INFO:root:Generator loss: 0.05789844428632155, Discriminator loss: 0.004424736179714176
INFO:root:
=== Epoch 26 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.3588,SSNR: 10.2470,CSIG: 4.5851,CBAK: 3.8604,COVL: 4.0676,STOI: 0.9540

Epoch 27/60 finished in 24.01 minutes
INFO:root:Epoch 27, Step 500, loss: 0.07481446862220764, disc_loss: 0.0019930775742977858
INFO:root:Epoch 27, Step 1000, loss: 0.07113109529018402, disc_loss: 0.0015231843572109938
INFO:root:Epoch 27, Step 1500, loss: 0.06795356422662735, disc_loss: 0.0018083092290908098
INFO:root:Epoch 27, Step 2000, loss: 0.07881723344326019, disc_loss: 0.002260711742565036
INFO:root:Epoch 27, Step 2500, loss: 0.07854975759983063, disc_loss: 0.0013327436754480004
INFO:root:Generator loss: 0.05819654613914131, Discriminator loss: 0.004834581474815688
INFO:root:
=== Epoch 27 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.3364,SSNR: 10.5216,CSIG: 4.5962,CBAK: 3.8683,COVL: 4.0631,STOI: 0.9540

Epoch 28/60 finished in 23.74 minutes
INFO:root:Epoch 28, Step 500, loss: 0.066983662545681, disc_loss: 0.003793108742684126
INFO:root:Epoch 28, Step 1000, loss: 0.060161277651786804, disc_loss: 0.001838582567870617
INFO:root:Epoch 28, Step 1500, loss: 0.13001510500907898, disc_loss: 0.0017346901586279273
INFO:root:Epoch 28, Step 2000, loss: 0.10127409547567368, disc_loss: 0.0002860364911612123
INFO:root:Epoch 28, Step 2500, loss: 0.08686564117670059, disc_loss: 0.0006053672987036407
INFO:root:Generator loss: 0.06032443747322247, Discriminator loss: 0.0060485232351161315
INFO:root:
=== Epoch 28 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.3133,SSNR: 9.9124,CSIG: 4.5736,CBAK: 3.8146,COVL: 4.0364,STOI: 0.9560

Epoch 29/60 finished in 23.82 minutes
INFO:root:Epoch 29, Step 500, loss: 0.07795926928520203, disc_loss: 0.001269920845516026
INFO:root:Epoch 29, Step 1000, loss: 0.07334356755018234, disc_loss: 0.0008374027092941105
INFO:root:Epoch 29, Step 1500, loss: 0.07259277254343033, disc_loss: 0.0010274008382111788
INFO:root:Epoch 29, Step 2000, loss: 0.052482377737760544, disc_loss: 0.000627483066637069
INFO:root:Epoch 29, Step 2500, loss: 0.05188654735684395, disc_loss: 0.0013076179893687367
INFO:root:Generator loss: 0.05933012394686636, Discriminator loss: 0.005448900271995158
INFO:root:
=== Epoch 29 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.3134,SSNR: 10.1392,CSIG: 4.6161,CBAK: 3.8313,COVL: 4.0595,STOI: 0.9566

Epoch 30/60 finished in 23.73 minutes
INFO:root:Epoch 30, Step 500, loss: 0.11059995740652084, disc_loss: 0.0022661383263766766
INFO:root:Epoch 30, Step 1000, loss: 0.08602861315011978, disc_loss: 0.0006445127655752003
INFO:root:Epoch 30, Step 1500, loss: 0.07530374079942703, disc_loss: 0.0011214783880859613
INFO:root:Epoch 30, Step 2000, loss: 0.07505852729082108, disc_loss: 0.0007942522061057389
INFO:root:Epoch 30, Step 2500, loss: 0.09195878356695175, disc_loss: 0.002231662394478917
INFO:root:Generator loss: 0.05715779658153798, Discriminator loss: 0.003995053154213362
INFO:root:
=== Epoch 30 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.3579,SSNR: 10.4727,CSIG: 4.6056,CBAK: 3.8745,COVL: 4.0779,STOI: 0.9551

Epoch 31/60 finished in 23.76 minutes
INFO:root:Epoch 31, Step 500, loss: 0.0702749639749527, disc_loss: 0.0005166087648831308
INFO:root:Epoch 31, Step 1000, loss: 0.0702664703130722, disc_loss: 0.0013311892980709672
INFO:root:Epoch 31, Step 1500, loss: 0.06632522493600845, disc_loss: 0.0014114661607891321
INFO:root:Epoch 31, Step 2000, loss: 0.047690339386463165, disc_loss: 0.0004119856748729944
INFO:root:Epoch 31, Step 2500, loss: 0.06424280256032944, disc_loss: 0.002803072100505233
INFO:root:Generator loss: 0.05759090521526568, Discriminator loss: 0.0036742355140393433
INFO:root:
=== Epoch 31 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.3463,SSNR: 10.4969,CSIG: 4.5963,CBAK: 3.8700,COVL: 4.0686,STOI: 0.9567

Epoch 32/60 finished in 23.62 minutes
INFO:root:Epoch 32, Step 500, loss: 0.09275070577859879, disc_loss: 0.0004644221335183829
INFO:root:Epoch 32, Step 1000, loss: 0.07955057919025421, disc_loss: 0.0019082605140283704
INFO:root:Epoch 32, Step 1500, loss: 0.07384153455495834, disc_loss: 0.0011572175426408648
INFO:root:Epoch 32, Step 2000, loss: 0.07550202310085297, disc_loss: 0.003061004215851426
INFO:root:Epoch 32, Step 2500, loss: 0.04640906676650047, disc_loss: 0.0010793348774313927
INFO:root:Generator loss: 0.05882251958896234, Discriminator loss: 0.006358797450922034
INFO:root:
=== Epoch 32 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.3556,SSNR: 10.1837,CSIG: 4.5891,CBAK: 3.8531,COVL: 4.0694,STOI: 0.9559

Epoch 33/60 finished in 23.68 minutes
INFO:root:Epoch 33, Step 500, loss: 0.05631006509065628, disc_loss: 0.005673252046108246
INFO:root:Epoch 33, Step 1000, loss: 0.047352392226457596, disc_loss: 0.0005508905160240829
INFO:root:Epoch 33, Step 1500, loss: 0.05262907221913338, disc_loss: 0.0016098476480692625
INFO:root:Epoch 33, Step 2000, loss: 0.10569433122873306, disc_loss: 0.002855670638382435
INFO:root:Epoch 33, Step 2500, loss: 0.08657184988260269, disc_loss: 0.0007176155922934413
INFO:root:Generator loss: 0.05853408555165657, Discriminator loss: 0.005326386665455201
INFO:root:
=== Epoch 33 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.3451,SSNR: 10.2944,CSIG: 4.5971,CBAK: 3.8554,COVL: 4.0663,STOI: 0.9560

Epoch 34/60 finished in 23.81 minutes
INFO:root:Epoch 34, Step 500, loss: 0.074720099568367, disc_loss: 0.003310869447886944
INFO:root:Epoch 34, Step 1000, loss: 0.06727904826402664, disc_loss: 0.0023291276302188635
INFO:root:Epoch 34, Step 1500, loss: 0.09466303139925003, disc_loss: 0.0034479363821446896
INFO:root:Epoch 34, Step 2000, loss: 0.06978687644004822, disc_loss: 0.0006062293541617692
INFO:root:Epoch 34, Step 2500, loss: 0.08421684056520462, disc_loss: 0.0006827956531196833
INFO:root:Generator loss: 0.05867195644786636, Discriminator loss: 0.004249124943933707
INFO:root:
=== Epoch 34 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.3439,SSNR: 10.3079,CSIG: 4.6128,CBAK: 3.8539,COVL: 4.0735,STOI: 0.9568

Epoch 35/60 finished in 23.61 minutes
INFO:root:Epoch 35, Step 500, loss: 0.07871299237012863, disc_loss: 0.002833146369084716
INFO:root:Epoch 35, Step 1000, loss: 0.0740099549293518, disc_loss: 0.0012583513744175434
INFO:root:Epoch 35, Step 1500, loss: 0.0767376571893692, disc_loss: 0.004429102409631014
INFO:root:Epoch 35, Step 2000, loss: 0.10213708132505417, disc_loss: 0.0015979793388396502
INFO:root:Epoch 35, Step 2500, loss: 0.08832300454378128, disc_loss: 0.0013128486461937428
INFO:root:Generator loss: 0.059786317336544825, Discriminator loss: 0.0051942188153580255
INFO:root:
=== Epoch 35 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.3423,SSNR: 10.0720,CSIG: 4.6022,CBAK: 3.8397,COVL: 4.0689,STOI: 0.9551

Epoch 36/60 finished in 23.59 minutes
INFO:root:Epoch 36, Step 500, loss: 0.06894083321094513, disc_loss: 0.0018991974648088217
INFO:root:Epoch 36, Step 1000, loss: 0.08367286622524261, disc_loss: 0.00288455025292933
INFO:root:Epoch 36, Step 1500, loss: 0.09105804562568665, disc_loss: 0.00033156279823742807
INFO:root:Epoch 36, Step 2000, loss: 0.10469893366098404, disc_loss: 0.0005317389732226729
INFO:root:Epoch 36, Step 2500, loss: 0.10174322128295898, disc_loss: 0.0006644521490670741
INFO:root:Generator loss: 0.057318070406112274, Discriminator loss: 0.005396881047452605
INFO:root:
=== Epoch 36 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.3476,SSNR: 10.4832,CSIG: 4.5976,CBAK: 3.8704,COVL: 4.0698,STOI: 0.9560

Epoch 37/60 finished in 23.75 minutes
INFO:root:Epoch 37, Step 500, loss: 0.11301607638597488, disc_loss: 0.0004959524376317859
INFO:root:Epoch 37, Step 1000, loss: 0.09706632047891617, disc_loss: 0.0007260669954121113
INFO:root:Epoch 37, Step 1500, loss: 0.08593585342168808, disc_loss: 0.0002519648987799883
INFO:root:Epoch 37, Step 2000, loss: 0.07667586207389832, disc_loss: 0.0005950302584096789
INFO:root:Epoch 37, Step 2500, loss: 0.06295427680015564, disc_loss: 0.0016877719899639487
INFO:root:Generator loss: 0.05819372222442361, Discriminator loss: 0.004862497980124741
INFO:root:
=== Epoch 37 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.3515,SSNR: 10.3615,CSIG: 4.6085,CBAK: 3.8620,COVL: 4.0772,STOI: 0.9562

Epoch 38/60 finished in 23.87 minutes
INFO:root:Epoch 38, Step 500, loss: 0.09833645075559616, disc_loss: 0.0006414274103008211
INFO:root:Epoch 38, Step 1000, loss: 0.06948860734701157, disc_loss: 0.005780416540801525
INFO:root:Epoch 38, Step 1500, loss: 0.03701306879520416, disc_loss: 0.0003713098994921893
INFO:root:Epoch 38, Step 2000, loss: 0.06671871244907379, disc_loss: 0.007934373803436756
INFO:root:Epoch 38, Step 2500, loss: 0.06498298794031143, disc_loss: 0.0010844102362170815
INFO:root:Generator loss: 0.057838756193235086, Discriminator loss: 0.003895633714104035
INFO:root:
=== Epoch 38 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.3863,SSNR: 10.5118,CSIG: 4.6111,CBAK: 3.8831,COVL: 4.0974,STOI: 0.9566

Epoch 39/60 finished in 24.22 minutes
INFO:root:Epoch 39, Step 500, loss: 0.08240401744842529, disc_loss: 0.0008794872555881739
INFO:root:Epoch 39, Step 1000, loss: 0.08790051192045212, disc_loss: 0.0021139844320714474
INFO:root:Epoch 39, Step 1500, loss: 0.06297771632671356, disc_loss: 0.000891402130946517
INFO:root:Epoch 39, Step 2000, loss: 0.06906528770923615, disc_loss: 0.00273360894061625
INFO:root:Epoch 39, Step 2500, loss: 0.067386195063591, disc_loss: 0.001070019556209445
INFO:root:Generator loss: 0.057300639266455636, Discriminator loss: 0.005019168596510161
INFO:root:
=== Epoch 39 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.3707,SSNR: 10.4096,CSIG: 4.5926,CBAK: 3.8827,COVL: 4.0797,STOI: 0.9561

Epoch 40/60 finished in 23.95 minutes
INFO:root:Epoch 40, Step 500, loss: 0.08807925879955292, disc_loss: 0.0011515386868268251
INFO:root:Epoch 40, Step 1000, loss: 0.07521442323923111, disc_loss: 0.0010561624076217413
INFO:root:Epoch 40, Step 1500, loss: 0.06486490368843079, disc_loss: 0.0012618715409189463
INFO:root:Epoch 40, Step 2000, loss: 0.08079466968774796, disc_loss: 0.001475370372645557
INFO:root:Epoch 40, Step 2500, loss: 0.09870041161775589, disc_loss: 0.0024401999544352293
INFO:root:Generator loss: 0.057819968686231134, Discriminator loss: 0.004269029271720516
INFO:root:
=== Epoch 40 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.3686,SSNR: 10.4250,CSIG: 4.6113,CBAK: 3.8749,COVL: 4.0873,STOI: 0.9565

Epoch 41/60 finished in 23.96 minutes
INFO:root:Epoch 41, Step 500, loss: 0.10982277244329453, disc_loss: 0.001165849156677723
INFO:root:Epoch 41, Step 1000, loss: 0.08901496231555939, disc_loss: 0.002411632100120187
INFO:root:Epoch 41, Step 1500, loss: 0.08614226430654526, disc_loss: 0.000647085253149271
INFO:root:Epoch 41, Step 2000, loss: 0.09435179084539413, disc_loss: 0.0014110296033322811
INFO:root:Epoch 41, Step 2500, loss: 0.06462723016738892, disc_loss: 0.001412650803104043
INFO:root:Generator loss: 0.059392180854400387, Discriminator loss: 0.004878629660488867
INFO:root:
=== Epoch 41 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.3111,SSNR: 10.4529,CSIG: 4.5835,CBAK: 3.8493,COVL: 4.0415,STOI: 0.9567

Epoch 42/60 finished in 24.06 minutes
INFO:root:Epoch 42, Step 500, loss: 0.05708739906549454, disc_loss: 0.0013535672333091497
INFO:root:Epoch 42, Step 1000, loss: 0.0687776580452919, disc_loss: 9.45720385061577e-05
INFO:root:Epoch 42, Step 1500, loss: 0.10588760673999786, disc_loss: 0.0033076154068112373
INFO:root:Epoch 42, Step 2000, loss: 0.048849958926439285, disc_loss: 0.0007462352514266968
INFO:root:Epoch 42, Step 2500, loss: 0.08023519814014435, disc_loss: 0.0012682690285146236
INFO:root:Generator loss: 0.057912118823348895, Discriminator loss: 0.004432471670031977
INFO:root:
=== Epoch 42 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.3578,SSNR: 10.5349,CSIG: 4.6032,CBAK: 3.8767,COVL: 4.0775,STOI: 0.9569

Epoch 43/60 finished in 23.96 minutes
INFO:root:Epoch 43, Step 500, loss: 0.06364606320858002, disc_loss: 0.0008082282729446888
INFO:root:Epoch 43, Step 1000, loss: 0.038823362439870834, disc_loss: 0.00018281866505276412
INFO:root:Epoch 43, Step 1500, loss: 0.11053171753883362, disc_loss: 0.0009580437326803803
INFO:root:Epoch 43, Step 2000, loss: 0.10051609575748444, disc_loss: 0.0007854730938561261
INFO:root:Epoch 43, Step 2500, loss: 0.038314394652843475, disc_loss: 0.0009221130167134106
INFO:root:Generator loss: 0.05898032630386862, Discriminator loss: 0.005198789169351814
INFO:root:
=== Epoch 43 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.3679,SSNR: 10.2582,CSIG: 4.6094,CBAK: 3.8641,COVL: 4.0879,STOI: 0.9558

Epoch 44/60 finished in 23.94 minutes
INFO:root:Epoch 44, Step 500, loss: 0.1077568531036377, disc_loss: 0.0013044729130342603
INFO:root:Epoch 44, Step 1000, loss: 0.06144113466143608, disc_loss: 0.0007849170360714197
INFO:root:Epoch 44, Step 1500, loss: 0.06673772633075714, disc_loss: 0.00023247046920005232
INFO:root:Epoch 44, Step 2000, loss: 0.10910985618829727, disc_loss: 0.001959152752533555
INFO:root:Epoch 44, Step 2500, loss: 0.0679716095328331, disc_loss: 0.0023691742680966854
INFO:root:Generator loss: 0.05826606576159162, Discriminator loss: 0.004777174688658385
INFO:root:
=== Epoch 44 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.3747,SSNR: 10.2762,CSIG: 4.6056,CBAK: 3.8674,COVL: 4.0898,STOI: 0.9567

Epoch 45/60 finished in 23.97 minutes
INFO:root:Epoch 45, Step 500, loss: 0.07152463495731354, disc_loss: 0.0007968893041834235
INFO:root:Epoch 45, Step 1000, loss: 0.08254320174455643, disc_loss: 0.0017715835710987449
INFO:root:Epoch 45, Step 1500, loss: 0.08914531022310257, disc_loss: 0.0031402416061609983
INFO:root:Epoch 45, Step 2000, loss: 0.0661400705575943, disc_loss: 0.0006017808918841183
INFO:root:Epoch 45, Step 2500, loss: 0.09544713795185089, disc_loss: 0.0013631805777549744
INFO:root:Generator loss: 0.058591276488619524, Discriminator loss: 0.00475959295769464
INFO:root:
=== Epoch 45 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.3681,SSNR: 10.3865,CSIG: 4.5878,CBAK: 3.8730,COVL: 4.0770,STOI: 0.9564

Epoch 46/60 finished in 24.01 minutes
INFO:root:Epoch 46, Step 500, loss: 0.09142732620239258, disc_loss: 0.0005311843124218285
INFO:root:Epoch 46, Step 1000, loss: 0.05868369713425636, disc_loss: 0.0008683281484991312
INFO:root:Epoch 46, Step 1500, loss: 0.05295170098543167, disc_loss: 0.0006939552258700132
INFO:root:Epoch 46, Step 2000, loss: 0.049992069602012634, disc_loss: 0.0017170585924759507
INFO:root:Epoch 46, Step 2500, loss: 0.09622083604335785, disc_loss: 0.0012102583423256874
INFO:root:Generator loss: 0.05806618740836394, Discriminator loss: 0.006694337081992592
INFO:root:
=== Epoch 46 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.3755,SSNR: 10.4219,CSIG: 4.6112,CBAK: 3.8772,COVL: 4.0919,STOI: 0.9563

Epoch 47/60 finished in 24.07 minutes
INFO:root:Epoch 47, Step 500, loss: 0.07929760962724686, disc_loss: 0.0007163051632232964
INFO:root:Epoch 47, Step 1000, loss: 0.05664807930588722, disc_loss: 0.0014904916752129793
INFO:root:Epoch 47, Step 1500, loss: 0.07843314856290817, disc_loss: 0.003933825995773077
INFO:root:Epoch 47, Step 2000, loss: 0.059563443064689636, disc_loss: 0.001124905189499259
INFO:root:Epoch 47, Step 2500, loss: 0.07338165491819382, disc_loss: 0.001377710374072194
INFO:root:Generator loss: 0.059215462486286764, Discriminator loss: 0.007387130773212033
INFO:root:
=== Epoch 47 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.3616,SSNR: 10.0328,CSIG: 4.5886,CBAK: 3.8453,COVL: 4.0725,STOI: 0.9560

Epoch 48/60 finished in 24.00 minutes
INFO:root:Epoch 48, Step 500, loss: 0.06853076070547104, disc_loss: 0.0018091751262545586
INFO:root:Epoch 48, Step 1000, loss: 0.15073034167289734, disc_loss: 0.000534056918695569
INFO:root:Epoch 48, Step 1500, loss: 0.05792156606912613, disc_loss: 0.008744229562580585
INFO:root:Epoch 48, Step 2000, loss: 0.07438445836305618, disc_loss: 0.0012658529449254274
INFO:root:Epoch 48, Step 2500, loss: 0.06932611763477325, disc_loss: 0.0007753846584819257
INFO:root:Generator loss: 0.05915832067432913, Discriminator loss: 0.005535110830342357
INFO:root:
=== Epoch 48 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.3384,SSNR: 10.3250,CSIG: 4.5884,CBAK: 3.8529,COVL: 4.0603,STOI: 0.9564

Epoch 49/60 finished in 24.30 minutes
INFO:root:Epoch 49, Step 500, loss: 0.09151418507099152, disc_loss: 0.0003600454074330628
INFO:root:Epoch 49, Step 1000, loss: 0.06861616671085358, disc_loss: 0.0014444398693740368
INFO:root:Epoch 49, Step 1500, loss: 0.10741473734378815, disc_loss: 0.0005371649167500436
INFO:root:Epoch 49, Step 2000, loss: 0.052607495337724686, disc_loss: 0.0022507701069116592
INFO:root:Epoch 49, Step 2500, loss: 0.08631572127342224, disc_loss: 3.197814294253476e-05
INFO:root:Generator loss: 0.05828688661703496, Discriminator loss: 0.005328364879266162
INFO:root:
=== Epoch 49 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.3497,SSNR: 10.3429,CSIG: 4.6029,CBAK: 3.8596,COVL: 4.0724,STOI: 0.9566

Epoch 50/60 finished in 24.19 minutes
INFO:root:Epoch 50, Step 500, loss: 0.05494027957320213, disc_loss: 0.0010467354441061616
INFO:root:Epoch 50, Step 1000, loss: 0.08794601261615753, disc_loss: 0.0006066790665499866
INFO:root:Epoch 50, Step 1500, loss: 0.07982862740755081, disc_loss: 0.00037206741399131715
INFO:root:Epoch 50, Step 2000, loss: 0.10414380580186844, disc_loss: 0.0009197068866342306
INFO:root:Epoch 50, Step 2500, loss: 0.07420697808265686, disc_loss: 0.001564871403388679
INFO:root:Generator loss: 0.05781105886783126, Discriminator loss: 0.004681989164947101
INFO:root:
=== Epoch 50 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.3576,SSNR: 10.4459,CSIG: 4.6045,CBAK: 3.8705,COVL: 4.0767,STOI: 0.9566

Epoch 51/60 finished in 24.09 minutes
INFO:root:Epoch 51, Step 500, loss: 0.08291218429803848, disc_loss: 0.0052293045446276665
INFO:root:Epoch 51, Step 1000, loss: 0.07438717782497406, disc_loss: 0.0001895371387945488
INFO:root:Epoch 51, Step 1500, loss: 0.05729018524289131, disc_loss: 0.0008563537849113345
INFO:root:Epoch 51, Step 2000, loss: 0.04907826706767082, disc_loss: 0.0005013797781430185
INFO:root:Epoch 51, Step 2500, loss: 0.08488044142723083, disc_loss: 0.001668607466854155
INFO:root:Generator loss: 0.059030402198578545, Discriminator loss: 0.004813232278783051
INFO:root:
=== Epoch 51 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.3307,SSNR: 10.3245,CSIG: 4.5905,CBAK: 3.8495,COVL: 4.0546,STOI: 0.9566

Epoch 52/60 finished in 24.15 minutes
INFO:root:Epoch 52, Step 500, loss: 0.07509496808052063, disc_loss: 0.0028310585767030716
INFO:root:Epoch 52, Step 1000, loss: 0.06327836215496063, disc_loss: 0.0006848454941064119
INFO:root:Epoch 52, Step 1500, loss: 0.06859898567199707, disc_loss: 0.001157786580733955
INFO:root:Epoch 52, Step 2000, loss: 0.07852201908826828, disc_loss: 0.0009206578251905739
INFO:root:Epoch 52, Step 2500, loss: 0.08422550559043884, disc_loss: 0.0006914696423336864
INFO:root:Generator loss: 0.058443788014062976, Discriminator loss: 0.005388677660254332
INFO:root:
=== Epoch 52 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.3808,SSNR: 10.1747,CSIG: 4.6110,CBAK: 3.8634,COVL: 4.0955,STOI: 0.9563

Epoch 53/60 finished in 24.40 minutes
INFO:root:Epoch 53, Step 500, loss: 0.07753794640302658, disc_loss: 0.0007422667695209384
INFO:root:Epoch 53, Step 1000, loss: 0.09291843324899673, disc_loss: 0.0007833287818357348
INFO:root:Epoch 53, Step 1500, loss: 0.11292169988155365, disc_loss: 0.001518359873443842
INFO:root:Epoch 53, Step 2000, loss: 0.06955861300230026, disc_loss: 0.0013645897852256894
INFO:root:Epoch 53, Step 2500, loss: 0.08532194793224335, disc_loss: 0.0033081546425819397
INFO:root:Generator loss: 0.058519944214054105, Discriminator loss: 0.00417645700203209
INFO:root:
=== Epoch 53 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.3307,SSNR: 10.4578,CSIG: 4.5844,CBAK: 3.8577,COVL: 4.0533,STOI: 0.9570

Epoch 54/60 finished in 24.31 minutes
INFO:root:Epoch 54, Step 500, loss: 0.09780006110668182, disc_loss: 0.000881717714946717
INFO:root:Epoch 54, Step 1000, loss: 0.09724296629428864, disc_loss: 0.0006041910964995623
INFO:root:Epoch 54, Step 1500, loss: 0.05527042597532272, disc_loss: 0.0010932510485872626
INFO:root:Epoch 54, Step 2000, loss: 0.10280275344848633, disc_loss: 0.0018875210080295801
INFO:root:Epoch 54, Step 2500, loss: 0.07563147693872452, disc_loss: 0.0008975803502835333
INFO:root:Generator loss: 0.05892244055505517, Discriminator loss: 0.0051628391874261265
INFO:root:
=== Epoch 54 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.3543,SSNR: 10.3042,CSIG: 4.5970,CBAK: 3.8587,COVL: 4.0713,STOI: 0.9564

Epoch 55/60 finished in 24.07 minutes
INFO:root:Epoch 55, Step 500, loss: 0.06425362080335617, disc_loss: 0.0020604950841516256
INFO:root:Epoch 55, Step 1000, loss: 0.085961252450943, disc_loss: 0.0016124739777296782
INFO:root:Epoch 55, Step 1500, loss: 0.04961838200688362, disc_loss: 0.0002519486879464239
INFO:root:Epoch 55, Step 2000, loss: 0.09152944386005402, disc_loss: 0.0017436603084206581
INFO:root:Epoch 55, Step 2500, loss: 0.09727182984352112, disc_loss: 0.0044397893361747265
INFO:root:Generator loss: 0.0582919796766008, Discriminator loss: 0.005345365090778994
INFO:root:
=== Epoch 55 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.3603,SSNR: 10.4038,CSIG: 4.5896,CBAK: 3.8668,COVL: 4.0692,STOI: 0.9565

Epoch 56/60 finished in 24.10 minutes
INFO:root:Epoch 56, Step 500, loss: 0.07539486140012741, disc_loss: 0.0004823968338314444
INFO:root:Epoch 56, Step 1000, loss: 0.08299463242292404, disc_loss: 0.0027173429261893034
INFO:root:Epoch 56, Step 1500, loss: 0.04542646184563637, disc_loss: 0.002058954443782568
INFO:root:Epoch 56, Step 2000, loss: 0.0828833356499672, disc_loss: 0.0013266318710520864
INFO:root:Epoch 56, Step 2500, loss: 0.08294433355331421, disc_loss: 0.0006330684991553426
INFO:root:Generator loss: 0.05863042418998711, Discriminator loss: 0.004847786466024383
INFO:root:
=== Epoch 56 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.3497,SSNR: 10.4784,CSIG: 4.5937,CBAK: 3.8700,COVL: 4.0705,STOI: 0.9568

Epoch 57/60 finished in 24.23 minutes
INFO:root:Epoch 57, Step 500, loss: 0.08319592475891113, disc_loss: 0.0017846336122602224
INFO:root:Epoch 57, Step 1000, loss: 0.07490115612745285, disc_loss: 0.0016548106214031577
INFO:root:Epoch 57, Step 1500, loss: 0.10703470557928085, disc_loss: 0.0030406457372009754
INFO:root:Epoch 57, Step 2000, loss: 0.07325487583875656, disc_loss: 0.00018743149121291935
INFO:root:Epoch 57, Step 2500, loss: 0.07362810522317886, disc_loss: 0.0005401130183599889
INFO:root:Generator loss: 0.059000986492604886, Discriminator loss: 0.0052700598963688165
INFO:root:
=== Epoch 57 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.3704,SSNR: 10.2679,CSIG: 4.5976,CBAK: 3.8626,COVL: 4.0828,STOI: 0.9562

Epoch 58/60 finished in 24.44 minutes
INFO:root:Epoch 58, Step 500, loss: 0.10112840682268143, disc_loss: 0.0003930055536329746
INFO:root:Epoch 58, Step 1000, loss: 0.10131216049194336, disc_loss: 0.0002731505664996803
INFO:root:Epoch 58, Step 1500, loss: 0.07064962387084961, disc_loss: 0.001082400674931705
INFO:root:Epoch 58, Step 2000, loss: 0.07147848606109619, disc_loss: 0.0028236620128154755
INFO:root:Epoch 58, Step 2500, loss: 0.06684362143278122, disc_loss: 0.0017737565794959664
INFO:root:Generator loss: 0.05924658279287294, Discriminator loss: 0.005268801066087637
INFO:root:
=== Epoch 58 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.3766,SSNR: 10.2203,CSIG: 4.6045,CBAK: 3.8610,COVL: 4.0879,STOI: 0.9560

Epoch 59/60 finished in 24.44 minutes
INFO:root:Epoch 59, Step 500, loss: 0.05807965248823166, disc_loss: 0.0005169254145585
INFO:root:Epoch 59, Step 1000, loss: 0.06553078442811966, disc_loss: 0.0002888625313062221
INFO:root:Epoch 59, Step 1500, loss: 0.06806427240371704, disc_loss: 0.0004435077717062086
INFO:root:Epoch 59, Step 2000, loss: 0.06750384718179703, disc_loss: 0.001303445198573172
INFO:root:Epoch 59, Step 2500, loss: 0.1005038321018219, disc_loss: 0.0010814083507284522
INFO:root:Generator loss: 0.0595264192851423, Discriminator loss: 0.007096877114695284
INFO:root:
=== Epoch 59 evaluation ===
INFO:root:=== Evaluation metrics ===
PESQ: 3.3395,SSNR: 10.2875,CSIG: 4.5907,CBAK: 3.8484,COVL: 4.0589,STOI: 0.9567

Epoch 60/60 finished in 24.48 minutes

Total training time: 27.60 hours
Training finished.



Starting training:
with SE:
Namespace(batch_size=4, cut_len=32000, data_dir='/mnt/iusers01/msc-stu/hum-msc-data-sci-2024-2025/t74061zq/erp/DEMAND_16KHz', decay_epoch=12, epochs=60, init_lr=0.0005, log_interval=500, loss_weights=[0.1, 0.9, 0.2, 0.05], save_model_dir='./saved_models_log/saved_models_20250720_VoiceDEMAND_16khz')
['NVIDIA A100-SXM4-80GB']
====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
TSCNet                                             [1, 1, 321, 201]          --
├─DenseEncoder: 1-1                                [1, 64, 321, 101]         --
│    └─Sequential: 2-1                             [1, 64, 321, 201]         --
│    │    └─Conv2d: 3-1                            [1, 64, 321, 201]         256
│    │    └─InstanceNorm2d: 3-2                    [1, 64, 321, 201]         128
│    │    └─PReLU: 3-3                             [1, 64, 321, 201]         64
│    └─SEBlock: 2-2                                [1, 64, 321, 201]         --
│    │    └─Sequential: 3-4                        [1, 64]                   580
│    └─DilatedDenseNet: 2-3                        [1, 64, 321, 201]         --
│    │    └─ConstantPad2d: 3-5                     [1, 64, 322, 203]         --
│    │    └─Conv2d: 3-6                            [1, 64, 321, 201]         24,640
│    │    └─InstanceNorm2d: 3-7                    [1, 64, 321, 201]         128
│    │    └─PReLU: 3-8                             [1, 64, 321, 201]         64
│    │    └─ConstantPad2d: 3-9                     [1, 128, 323, 203]        --
│    │    └─Conv2d: 3-10                           [1, 64, 321, 201]         49,216
│    │    └─InstanceNorm2d: 3-11                   [1, 64, 321, 201]         128
│    │    └─PReLU: 3-12                            [1, 64, 321, 201]         64
│    │    └─ConstantPad2d: 3-13                    [1, 192, 325, 203]        --
│    │    └─Conv2d: 3-14                           [1, 64, 321, 201]         73,792
│    │    └─InstanceNorm2d: 3-15                   [1, 64, 321, 201]         128
│    │    └─PReLU: 3-16                            [1, 64, 321, 201]         64
│    │    └─ConstantPad2d: 3-17                    [1, 256, 329, 203]        --
│    │    └─Conv2d: 3-18                           [1, 64, 321, 201]         98,368
│    │    └─InstanceNorm2d: 3-19                   [1, 64, 321, 201]         128
│    │    └─PReLU: 3-20                            [1, 64, 321, 201]         64
│    └─Sequential: 2-4                             [1, 64, 321, 101]         --
│    │    └─Conv2d: 3-21                           [1, 64, 321, 101]         12,352
│    │    └─InstanceNorm2d: 3-22                   [1, 64, 321, 101]         128
│    │    └─PReLU: 3-23                            [1, 64, 321, 101]         64
├─TSCB: 1-2                                        [1, 64, 321, 101]         --
│    └─ConformerBlock: 2-5                         [101, 321, 64]            --
│    │    └─Scale: 3-24                            [101, 321, 64]            33,216
│    │    └─PreNorm: 3-25                          [101, 321, 64]            32,976
│    │    └─ConformerConvModule: 3-26              [101, 321, 64]            29,376
│    │    └─Scale: 3-27                            [101, 321, 64]            33,216
│    │    └─LayerNorm: 3-28                        [101, 321, 64]            128
│    └─ConformerBlock: 2-6                         [321, 101, 64]            --
│    │    └─Scale: 3-29                            [321, 101, 64]            33,216
│    │    └─PreNorm: 3-30                          [321, 101, 64]            32,976
│    │    └─ConformerConvModule: 3-31              [321, 101, 64]            29,376
│    │    └─Scale: 3-32                            [321, 101, 64]            33,216
│    │    └─LayerNorm: 3-33                        [321, 101, 64]            128
├─TSCB: 1-3                                        [1, 64, 321, 101]         --
│    └─ConformerBlock: 2-7                         [101, 321, 64]            --
│    │    └─Scale: 3-34                            [101, 321, 64]            33,216
│    │    └─PreNorm: 3-35                          [101, 321, 64]            32,976
│    │    └─ConformerConvModule: 3-36              [101, 321, 64]            29,376
│    │    └─Scale: 3-37                            [101, 321, 64]            33,216
│    │    └─LayerNorm: 3-38                        [101, 321, 64]            128
│    └─ConformerBlock: 2-8                         [321, 101, 64]            --
│    │    └─Scale: 3-39                            [321, 101, 64]            33,216
│    │    └─PreNorm: 3-40                          [321, 101, 64]            32,976
│    │    └─ConformerConvModule: 3-41              [321, 101, 64]            29,376
│    │    └─Scale: 3-42                            [321, 101, 64]            33,216
│    │    └─LayerNorm: 3-43                        [321, 101, 64]            128
├─TSCB: 1-4                                        [1, 64, 321, 101]         --
│    └─ConformerBlock: 2-9                         [101, 321, 64]            --
│    │    └─Scale: 3-44                            [101, 321, 64]            33,216
│    │    └─PreNorm: 3-45                          [101, 321, 64]            32,976
│    │    └─ConformerConvModule: 3-46              [101, 321, 64]            29,376
│    │    └─Scale: 3-47                            [101, 321, 64]            33,216
│    │    └─LayerNorm: 3-48                        [101, 321, 64]            128
│    └─ConformerBlock: 2-10                        [321, 101, 64]            --
│    │    └─Scale: 3-49                            [321, 101, 64]            33,216
│    │    └─PreNorm: 3-50                          [321, 101, 64]            32,976
│    │    └─ConformerConvModule: 3-51              [321, 101, 64]            29,376
│    │    └─Scale: 3-52                            [321, 101, 64]            33,216
│    │    └─LayerNorm: 3-53                        [321, 101, 64]            128
├─TSCB: 1-5                                        [1, 64, 321, 101]         --
│    └─ConformerBlock: 2-11                        [101, 321, 64]            --
│    │    └─Scale: 3-54                            [101, 321, 64]            33,216
│    │    └─PreNorm: 3-55                          [101, 321, 64]            32,976
│    │    └─ConformerConvModule: 3-56              [101, 321, 64]            29,376
│    │    └─Scale: 3-57                            [101, 321, 64]            33,216
│    │    └─LayerNorm: 3-58                        [101, 321, 64]            128
│    └─ConformerBlock: 2-12                        [321, 101, 64]            --
│    │    └─Scale: 3-59                            [321, 101, 64]            33,216
│    │    └─PreNorm: 3-60                          [321, 101, 64]            32,976
│    │    └─ConformerConvModule: 3-61              [321, 101, 64]            29,376
│    │    └─Scale: 3-62                            [321, 101, 64]            33,216
│    │    └─LayerNorm: 3-63                        [321, 101, 64]            128
├─MaskDecoder: 1-6                                 [1, 1, 321, 201]          --
│    └─DilatedDenseNet: 2-13                       [1, 64, 321, 101]         --
│    │    └─ConstantPad2d: 3-64                    [1, 64, 322, 103]         --
│    │    └─Conv2d: 3-65                           [1, 64, 321, 101]         24,640
│    │    └─InstanceNorm2d: 3-66                   [1, 64, 321, 101]         128
│    │    └─PReLU: 3-67                            [1, 64, 321, 101]         64
│    │    └─ConstantPad2d: 3-68                    [1, 128, 323, 103]        --
│    │    └─Conv2d: 3-69                           [1, 64, 321, 101]         49,216
│    │    └─InstanceNorm2d: 3-70                   [1, 64, 321, 101]         128
│    │    └─PReLU: 3-71                            [1, 64, 321, 101]         64
│    │    └─ConstantPad2d: 3-72                    [1, 192, 325, 103]        --
│    │    └─Conv2d: 3-73                           [1, 64, 321, 101]         73,792
│    │    └─InstanceNorm2d: 3-74                   [1, 64, 321, 101]         128
│    │    └─PReLU: 3-75                            [1, 64, 321, 101]         64
│    │    └─ConstantPad2d: 3-76                    [1, 256, 329, 103]        --
│    │    └─Conv2d: 3-77                           [1, 64, 321, 101]         98,368
│    │    └─InstanceNorm2d: 3-78                   [1, 64, 321, 101]         128
│    │    └─PReLU: 3-79                            [1, 64, 321, 101]         64
│    └─SEBlock: 2-14                               [1, 64, 321, 101]         --
│    │    └─Sequential: 3-80                       [1, 64]                   580
│    └─SPConvTranspose2d: 2-15                     [1, 64, 321, 202]         --
│    │    └─ConstantPad2d: 3-81                    [1, 64, 321, 103]         --
│    │    └─Conv2d: 3-82                           [1, 128, 321, 101]        24,704
│    └─Conv2d: 2-16                                [1, 1, 321, 201]          129
│    └─InstanceNorm2d: 2-17                        [1, 1, 321, 201]          2
│    └─PReLU: 2-18                                 [1, 1, 321, 201]          1
│    └─Conv2d: 2-19                                [1, 1, 321, 201]          2
│    └─PReLU: 2-20                                 [1, 201, 321]             201
├─ComplexDecoder: 1-7                              [1, 2, 321, 201]          --
│    └─DilatedDenseNet: 2-21                       [1, 64, 321, 101]         --
│    │    └─ConstantPad2d: 3-83                    [1, 64, 322, 103]         --
│    │    └─Conv2d: 3-84                           [1, 64, 321, 101]         24,640
│    │    └─InstanceNorm2d: 3-85                   [1, 64, 321, 101]         128
│    │    └─PReLU: 3-86                            [1, 64, 321, 101]         64
│    │    └─ConstantPad2d: 3-87                    [1, 128, 323, 103]        --
│    │    └─Conv2d: 3-88                           [1, 64, 321, 101]         49,216
│    │    └─InstanceNorm2d: 3-89                   [1, 64, 321, 101]         128
│    │    └─PReLU: 3-90                            [1, 64, 321, 101]         64
│    │    └─ConstantPad2d: 3-91                    [1, 192, 325, 103]        --
│    │    └─Conv2d: 3-92                           [1, 64, 321, 101]         73,792
│    │    └─InstanceNorm2d: 3-93                   [1, 64, 321, 101]         128
│    │    └─PReLU: 3-94                            [1, 64, 321, 101]         64
│    │    └─ConstantPad2d: 3-95                    [1, 256, 329, 103]        --
│    │    └─Conv2d: 3-96                           [1, 64, 321, 101]         98,368
│    │    └─InstanceNorm2d: 3-97                   [1, 64, 321, 101]         128
│    │    └─PReLU: 3-98                            [1, 64, 321, 101]         64
│    └─SEBlock: 2-22                               [1, 64, 321, 101]         --
│    │    └─Sequential: 3-99                       [1, 64]                   580
│    └─SPConvTranspose2d: 2-23                     [1, 64, 321, 202]         --
│    │    └─ConstantPad2d: 3-100                   [1, 64, 321, 103]         --
│    │    └─Conv2d: 3-101                          [1, 128, 321, 101]        24,704
│    └─InstanceNorm2d: 2-24                        [1, 64, 321, 202]         128
│    └─PReLU: 2-25                                 [1, 64, 321, 202]         64
│    └─Conv2d: 2-26                                [1, 2, 321, 201]          258
====================================================================================================
Total params: 1,836,573
Trainable params: 1,836,573
Non-trainable params: 0
Total mult-adds (G): 41.56
====================================================================================================
Input size (MB): 0.52
Forward/backward pass size (MB): 4856.40
Params size (MB): 7.35
Estimated Total Size (MB): 4864.26
====================================================================================================
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
Discriminator                            [1, 1]                    --
├─Sequential: 1-1                        [1, 1]                    --
│    └─Conv2d: 2-1                       [1, 16, 100, 160]         512
│    └─InstanceNorm2d: 2-2               [1, 16, 100, 160]         32
│    └─PReLU: 2-3                        [1, 16, 100, 160]         16
│    └─Conv2d: 2-4                       [1, 32, 50, 80]           8,192
│    └─InstanceNorm2d: 2-5               [1, 32, 50, 80]           64
│    └─PReLU: 2-6                        [1, 32, 50, 80]           32
│    └─Conv2d: 2-7                       [1, 64, 25, 40]           32,768
│    └─InstanceNorm2d: 2-8               [1, 64, 25, 40]           128
│    └─PReLU: 2-9                        [1, 64, 25, 40]           64
│    └─Conv2d: 2-10                      [1, 128, 12, 20]          131,072
│    └─InstanceNorm2d: 2-11              [1, 128, 12, 20]          256
│    └─PReLU: 2-12                       [1, 128, 12, 20]          128
│    └─AdaptiveMaxPool2d: 2-13           [1, 128, 1, 1]            --
│    └─Flatten: 2-14                     [1, 128]                  --
│    └─Linear: 2-15                      [1, 64]                   8,256
│    └─Dropout: 2-16                     [1, 64]                   --
│    └─PReLU: 2-17                       [1, 64]                   64
│    └─Linear: 2-18                      [1, 1]                    65
│    └─LearnableSigmoid: 2-19            [1, 1]                    1
==========================================================================================
Total params: 181,650
Trainable params: 181,650
Non-trainable params: 0
Total mult-adds (M): 19.67
==========================================================================================
Input size (MB): 0.52
Forward/backward pass size (MB): 11.49
Params size (MB): 0.73
Estimated Total Size (MB): 12.73
==========================================================================================
/mnt/iusers01/msc-stu/hum-msc-data-sci-2024-2025/t74061zq/erp/CMGAN_src/data/dataloader.py:52: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend("sox_io")         # in linux
INFO:root:Epoch 0, Step 500, loss: 0.08670128136873245, disc_loss: 0.04549947381019592
INFO:root:Epoch 0, Step 1000, loss: 0.15383672714233398, disc_loss: 0.0022508257534354925
INFO:root:Epoch 0, Step 1500, loss: 0.12372221797704697, disc_loss: 0.017925653606653214
INFO:root:Epoch 0, Step 2000, loss: 0.18941760063171387, disc_loss: 0.014190060086548328
INFO:root:Epoch 0, Step 2500, loss: 0.08496297150850296, disc_loss: 0.03749791905283928
INFO:root:Generator loss: 0.08291850989521707, Discriminator loss: 0.0060162141616654004
INFO:root:Epoch 1, Step 500, loss: 0.09048350900411606, disc_loss: 0.008999340236186981
INFO:root:Epoch 1, Step 1000, loss: 0.1376453936100006, disc_loss: 0.0037629480939358473
INFO:root:Epoch 1, Step 1500, loss: 0.11865082383155823, disc_loss: 0.002120917895808816
INFO:root:Epoch 1, Step 2000, loss: 0.13195064663887024, disc_loss: 0.005263976287096739
INFO:root:Epoch 1, Step 2500, loss: 0.13385486602783203, disc_loss: 0.005881730001419783
INFO:root:Generator loss: 0.07653015976753628, Discriminator loss: 0.013425401992331828
INFO:root:Epoch 2, Step 500, loss: 0.1251046061515808, disc_loss: 0.011628320440649986
INFO:root:Epoch 2, Step 1000, loss: 0.09123893082141876, disc_loss: 0.0043900227174162865
INFO:root:Epoch 2, Step 1500, loss: 0.11656910181045532, disc_loss: 0.0005681101465597749
INFO:root:Epoch 2, Step 2000, loss: 0.10369417071342468, disc_loss: 0.009141392074525356
INFO:root:Epoch 2, Step 2500, loss: 0.09433319419622421, disc_loss: 0.010555301792919636
INFO:root:Generator loss: 0.07670289892884134, Discriminator loss: 0.006191651526659397
INFO:root:Epoch 3, Step 500, loss: 0.07365970313549042, disc_loss: 0.006222601514309645
INFO:root:Epoch 3, Step 1000, loss: 0.06640885770320892, disc_loss: 0.0024116504937410355
INFO:root:Epoch 3, Step 1500, loss: 0.09020954370498657, disc_loss: 0.004300054628401995
INFO:root:Epoch 3, Step 2000, loss: 0.07639354467391968, disc_loss: 0.005803040694445372
INFO:root:Epoch 3, Step 2500, loss: 0.07741605490446091, disc_loss: 0.004015754442662001
INFO:root:Generator loss: 0.07142463778552499, Discriminator loss: 0.010368012455269441
INFO:root:Epoch 4, Step 500, loss: 0.060868751257658005, disc_loss: 0.0021687853150069714
INFO:root:Epoch 4, Step 1000, loss: 0.09985975921154022, disc_loss: 0.006139358039945364
INFO:root:Epoch 4, Step 1500, loss: 0.10898585617542267, disc_loss: 0.0040971036069095135
INFO:root:Epoch 4, Step 2000, loss: 0.1016320288181305, disc_loss: 0.002804334042593837
INFO:root:Epoch 4, Step 2500, loss: 0.0728202760219574, disc_loss: 0.003187736263498664
INFO:root:Generator loss: 0.07222755926037297, Discriminator loss: 0.015440156168781348
INFO:root:Epoch 5, Step 500, loss: 0.10090957581996918, disc_loss: 0.006334209349006414
INFO:root:Epoch 5, Step 1000, loss: 0.11728094518184662, disc_loss: 0.004116943571716547
INFO:root:Epoch 5, Step 1500, loss: 0.1221829205751419, disc_loss: 0.001470357645303011
INFO:root:Epoch 5, Step 2000, loss: 0.06969467550516129, disc_loss: 0.0007349690422415733
INFO:root:Epoch 5, Step 2500, loss: 0.11609116196632385, disc_loss: 0.004671248607337475
INFO:root:Generator loss: 0.06995886896999137, Discriminator loss: 0.005806805914236745
INFO:root:Epoch 6, Step 500, loss: 0.10180829465389252, disc_loss: 0.004842774942517281
INFO:root:Epoch 6, Step 1000, loss: 0.09816021472215652, disc_loss: 0.004481836687773466
INFO:root:Epoch 6, Step 1500, loss: 0.09276551753282547, disc_loss: 0.004897504113614559
INFO:root:Epoch 6, Step 2000, loss: 0.11109812557697296, disc_loss: 0.00277158641256392
INFO:root:Epoch 6, Step 2500, loss: 0.07126130163669586, disc_loss: 0.00127068639267236
INFO:root:Generator loss: 0.06497617083086261, Discriminator loss: 0.0048501542152682075
INFO:root:Epoch 7, Step 500, loss: 0.09285751730203629, disc_loss: 0.0011580548016354442
INFO:root:Epoch 7, Step 1000, loss: 0.1535399854183197, disc_loss: 0.0024081929586827755
INFO:root:Epoch 7, Step 1500, loss: 0.08319114148616791, disc_loss: 0.00654654111713171
INFO:root:Epoch 7, Step 2000, loss: 0.07489345967769623, disc_loss: 0.0055369725450873375
INFO:root:Epoch 7, Step 2500, loss: 0.08028696477413177, disc_loss: 0.0005299233016557992
INFO:root:Generator loss: 0.06665655664954949, Discriminator loss: 0.00565129646152519
INFO:root:Epoch 8, Step 500, loss: 0.07689709216356277, disc_loss: 0.0015063300961628556
INFO:root:Epoch 8, Step 1000, loss: 0.07494530826807022, disc_loss: 0.03422035276889801
INFO:root:Epoch 8, Step 1500, loss: 0.11364007741212845, disc_loss: 0.002225864678621292
INFO:root:Epoch 8, Step 2000, loss: 0.13930003345012665, disc_loss: 0.002463958691805601
INFO:root:Epoch 8, Step 2500, loss: 0.09919270873069763, disc_loss: 0.0008190099033527076
INFO:root:Generator loss: 0.06433217828749742, Discriminator loss: 0.006032953253946916
INFO:root:Epoch 9, Step 500, loss: 0.11734487861394882, disc_loss: 0.0008789430721662939
INFO:root:Epoch 9, Step 1000, loss: 0.12954068183898926, disc_loss: 0.0023873017635196447
INFO:root:Epoch 9, Step 1500, loss: 0.0878683552145958, disc_loss: 0.001668525394052267
INFO:root:Epoch 9, Step 2000, loss: 0.11113062500953674, disc_loss: 0.0005191471427679062
INFO:root:Epoch 9, Step 2500, loss: 0.08340948820114136, disc_loss: 0.0006709936424158514
INFO:root:Generator loss: 0.06583876170000984, Discriminator loss: 0.00585944441086963
INFO:root:Epoch 10, Step 500, loss: 0.10216347873210907, disc_loss: 0.0034612338058650494
INFO:root:Epoch 10, Step 1000, loss: 0.14420641958713531, disc_loss: 0.002839592518284917
INFO:root:Epoch 10, Step 1500, loss: 0.09776674956083298, disc_loss: 0.0018755675991997123
INFO:root:Epoch 10, Step 2000, loss: 0.0745086520910263, disc_loss: 0.0024487022310495377
INFO:root:Epoch 10, Step 2500, loss: 0.09530128538608551, disc_loss: 0.0005701774498447776
INFO:root:Generator loss: 0.06351389343515762, Discriminator loss: 0.004621718930209381
INFO:root:Epoch 11, Step 500, loss: 0.05691125988960266, disc_loss: 0.0008552863146178424
INFO:root:Epoch 11, Step 1000, loss: 0.13863737881183624, disc_loss: 0.002572439145296812
INFO:root:Epoch 11, Step 1500, loss: 0.11717037111520767, disc_loss: 0.0027056559920310974
INFO:root:Epoch 11, Step 2000, loss: 0.1158040389418602, disc_loss: 0.0035712679382413626
INFO:root:Epoch 11, Step 2500, loss: 0.05569004267454147, disc_loss: 0.0025253458879888058
INFO:root:Generator loss: 0.06164404327610453, Discriminator loss: 0.005919514718345136
INFO:root:Epoch 12, Step 500, loss: 0.06726498156785965, disc_loss: 0.003968609031289816
INFO:root:Epoch 12, Step 1000, loss: 0.0734231024980545, disc_loss: 0.0024395305663347244
INFO:root:Epoch 12, Step 1500, loss: 0.08302213251590729, disc_loss: 0.0019772376399487257
INFO:root:Epoch 12, Step 2000, loss: 0.05160926282405853, disc_loss: 0.0006879607099108398
INFO:root:Epoch 12, Step 2500, loss: 0.1026030108332634, disc_loss: 0.0006691347807645798
INFO:root:Generator loss: 0.05931138237305347, Discriminator loss: 0.006761740140523083
INFO:root:Epoch 13, Step 500, loss: 0.0947132557630539, disc_loss: 0.0016229323809966445
INFO:root:Epoch 13, Step 1000, loss: 0.0697564035654068, disc_loss: 0.002502958755940199
INFO:root:Epoch 13, Step 1500, loss: 0.07294803857803345, disc_loss: 0.0022550595458596945
INFO:root:Epoch 13, Step 2000, loss: 0.05482431873679161, disc_loss: 0.0008615636033937335
INFO:root:Epoch 13, Step 2500, loss: 0.09053369611501694, disc_loss: 0.001169466064311564
INFO:root:Generator loss: 0.05962528376975684, Discriminator loss: 0.006257189386744658
INFO:root:Epoch 14, Step 500, loss: 0.07108443230390549, disc_loss: 0.0022185416892170906
INFO:root:Epoch 14, Step 1000, loss: 0.08866633474826813, disc_loss: 0.000496160238981247
INFO:root:Epoch 14, Step 1500, loss: 0.09995649755001068, disc_loss: 0.0018282296368852258
INFO:root:Epoch 14, Step 2000, loss: 0.07946578413248062, disc_loss: 0.0007886941893957555
INFO:root:Epoch 14, Step 2500, loss: 0.06006031110882759, disc_loss: 0.0006916193524375558
INFO:root:Generator loss: 0.060670767033707745, Discriminator loss: 0.005573372571834802
INFO:root:Epoch 15, Step 500, loss: 0.07346697151660919, disc_loss: 0.002136718016117811
INFO:root:Epoch 15, Step 1000, loss: 0.07433722913265228, disc_loss: 0.0009708767756819725
INFO:root:Epoch 15, Step 1500, loss: 0.10258808732032776, disc_loss: 0.0007809172966517508
INFO:root:Epoch 15, Step 2000, loss: 0.06136716902256012, disc_loss: 0.0018919012509286404
INFO:root:Epoch 15, Step 2500, loss: 0.08703448623418808, disc_loss: 0.001771276700310409
INFO:root:Generator loss: 0.05867863306323591, Discriminator loss: 0.004422428349888595
INFO:root:Epoch 16, Step 500, loss: 0.09431303292512894, disc_loss: 0.008639855310320854
INFO:root:Epoch 16, Step 1000, loss: 0.11318263411521912, disc_loss: 0.0024837993551045656
INFO:root:Epoch 16, Step 1500, loss: 0.06780723482370377, disc_loss: 0.005243928637355566
INFO:root:Epoch 16, Step 2000, loss: 0.10205914825201035, disc_loss: 0.0006751047912985086
INFO:root:Epoch 16, Step 2500, loss: 0.07760203629732132, disc_loss: 0.002433250891044736
INFO:root:Generator loss: 0.05909035602602565, Discriminator loss: 0.008108877721417794
INFO:root:Epoch 17, Step 500, loss: 0.07118359208106995, disc_loss: 0.001018726616166532
INFO:root:Epoch 17, Step 1000, loss: 0.07049266993999481, disc_loss: 0.0003276981005910784
INFO:root:Epoch 17, Step 1500, loss: 0.07907910645008087, disc_loss: 0.0020139559637755156
INFO:root:Epoch 17, Step 2000, loss: 0.09613337367773056, disc_loss: 0.0008353170705959201
INFO:root:Epoch 17, Step 2500, loss: 0.0768222063779831, disc_loss: 0.0010305355535820127
INFO:root:Generator loss: 0.058413155394995094, Discriminator loss: 0.003934068052472267
INFO:root:Epoch 18, Step 500, loss: 0.10066744685173035, disc_loss: 0.0026463267859071493
INFO:root:Epoch 18, Step 1000, loss: 0.09346140921115875, disc_loss: 0.0034462616313248873
INFO:root:Epoch 18, Step 1500, loss: 0.04582483321428299, disc_loss: 0.0015749539015814662
INFO:root:Epoch 18, Step 2000, loss: 0.04998571053147316, disc_loss: 0.0011739658657461405
INFO:root:Epoch 18, Step 2500, loss: 0.07974780350923538, disc_loss: 0.0030303653329610825
INFO:root:Generator loss: 0.05845420570700493, Discriminator loss: 0.0036333091885503606
INFO:root:Epoch 19, Step 500, loss: 0.04600243642926216, disc_loss: 0.0005097786197438836
INFO:root:Epoch 19, Step 1000, loss: 0.07044699788093567, disc_loss: 0.0026674733962863684
INFO:root:Epoch 19, Step 1500, loss: 0.06875679641962051, disc_loss: 0.0013812422985211015
INFO:root:Epoch 19, Step 2000, loss: 0.06469395011663437, disc_loss: 0.0024428695905953646
INFO:root:Epoch 19, Step 2500, loss: 0.06773768365383148, disc_loss: 0.0008332555880770087
INFO:root:Generator loss: 0.05947085635429158, Discriminator loss: 0.00427683226279468
INFO:root:Epoch 20, Step 500, loss: 0.06281574070453644, disc_loss: 0.0024620299227535725
INFO:root:Epoch 20, Step 1000, loss: 0.11284735798835754, disc_loss: 0.0006138571770861745
INFO:root:Epoch 20, Step 1500, loss: 0.08344040811061859, disc_loss: 0.001956441206857562
INFO:root:Epoch 20, Step 2000, loss: 0.10634761303663254, disc_loss: 0.002102266065776348
INFO:root:Epoch 20, Step 2500, loss: 0.05351988226175308, disc_loss: 0.0009122418123297393
INFO:root:Generator loss: 0.0591274989425268, Discriminator loss: 0.0045599001169798654
INFO:root:Epoch 21, Step 500, loss: 0.0772707536816597, disc_loss: 0.0006975987926125526
INFO:root:Epoch 21, Step 1000, loss: 0.07880093157291412, disc_loss: 0.0005949294427409768
INFO:root:Epoch 21, Step 1500, loss: 0.08514974266290665, disc_loss: 0.0021075746044516563
INFO:root:Epoch 21, Step 2000, loss: 0.07340500503778458, disc_loss: 0.0005475725047290325
INFO:root:Epoch 21, Step 2500, loss: 0.10773739963769913, disc_loss: 0.0013337002601474524
INFO:root:Generator loss: 0.057524177860649464, Discriminator loss: 0.005554134700538023
INFO:root:Epoch 22, Step 500, loss: 0.06218436732888222, disc_loss: 0.000834885926451534
INFO:root:Epoch 22, Step 1000, loss: 0.06931599974632263, disc_loss: 0.0033642638009041548
INFO:root:Epoch 22, Step 1500, loss: 0.10372631251811981, disc_loss: 0.0017932786140590906
INFO:root:Epoch 22, Step 2000, loss: 0.06867785006761551, disc_loss: 0.0014653434045612812
INFO:root:Epoch 22, Step 2500, loss: 0.10683505237102509, disc_loss: 0.006068812217563391
INFO:root:Generator loss: 0.058897983626399225, Discriminator loss: 0.004821412047022022
INFO:root:Epoch 23, Step 500, loss: 0.06283705681562424, disc_loss: 0.00038638440310023725
INFO:root:Epoch 23, Step 1000, loss: 0.10053528100252151, disc_loss: 0.0007793242111802101
INFO:root:Epoch 23, Step 1500, loss: 0.07479502260684967, disc_loss: 0.00207327539101243
INFO:root:Epoch 23, Step 2000, loss: 0.0480530820786953, disc_loss: 0.0010863876668736339
INFO:root:Epoch 23, Step 2500, loss: 0.061389900743961334, disc_loss: 0.0025548236444592476
INFO:root:Generator loss: 0.06023731938952092, Discriminator loss: 0.005616193439070309
INFO:root:Epoch 24, Step 500, loss: 0.06298616528511047, disc_loss: 0.0018069627694785595
INFO:root:Epoch 24, Step 1000, loss: 0.06775111705064774, disc_loss: 0.0008139156852848828
INFO:root:Epoch 24, Step 1500, loss: 0.08057140558958054, disc_loss: 0.0016064861556515098
INFO:root:Epoch 24, Step 2000, loss: 0.06332913786172867, disc_loss: 5.399339715950191e-05
INFO:root:Epoch 24, Step 2500, loss: 0.09019947797060013, disc_loss: 0.0008735202136449516
INFO:root:Generator loss: 0.05748120162780713, Discriminator loss: 0.00403556118382184
INFO:root:Epoch 25, Step 500, loss: 0.09127967059612274, disc_loss: 0.0048397802747786045
INFO:root:Epoch 25, Step 1000, loss: 0.05418021231889725, disc_loss: 0.0006964401109144092
INFO:root:Epoch 25, Step 1500, loss: 0.06859208643436432, disc_loss: 0.0003860244760289788
INFO:root:Epoch 25, Step 2000, loss: 0.07583077996969223, disc_loss: 0.0002258227759739384
INFO:root:Epoch 25, Step 2500, loss: 0.09074307233095169, disc_loss: 0.001950979814864695
INFO:root:Generator loss: 0.05799985157090773, Discriminator loss: 0.002924424191714757
INFO:root:Epoch 26, Step 500, loss: 0.10732904076576233, disc_loss: 0.003988661337643862
INFO:root:Epoch 26, Step 1000, loss: 0.08182456344366074, disc_loss: 0.0007081814110279083
INFO:root:Epoch 26, Step 1500, loss: 0.10926764458417892, disc_loss: 0.0012069221120327711
INFO:root:Epoch 26, Step 2000, loss: 0.06650825589895248, disc_loss: 0.0018488921923562884
INFO:root:Epoch 26, Step 2500, loss: 0.12603935599327087, disc_loss: 0.00032415727037005126
INFO:root:Generator loss: 0.05707774934295601, Discriminator loss: 0.0040518964284556825
INFO:root:Epoch 27, Step 500, loss: 0.05546482652425766, disc_loss: 0.0009002723381854594
INFO:root:Epoch 27, Step 1000, loss: 0.09822066873311996, disc_loss: 0.0004915976896882057
INFO:root:Epoch 27, Step 1500, loss: 0.06322389096021652, disc_loss: 0.001507564214989543
INFO:root:Epoch 27, Step 2000, loss: 0.07355480641126633, disc_loss: 0.0012325242860242724
INFO:root:Epoch 27, Step 2500, loss: 0.07246123999357224, disc_loss: 0.0017789591802284122
INFO:root:Generator loss: 0.05749568386563977, Discriminator loss: 0.005970720770562269
INFO:root:Epoch 28, Step 500, loss: 0.0732610747218132, disc_loss: 0.00018431757052894682
INFO:root:Epoch 28, Step 1000, loss: 0.0925661250948906, disc_loss: 0.00035287541686557233
INFO:root:Epoch 28, Step 1500, loss: 0.06857156753540039, disc_loss: 0.0027448111213743687
INFO:root:Epoch 28, Step 2000, loss: 0.06176270917057991, disc_loss: 0.000916399119887501
INFO:root:Epoch 28, Step 2500, loss: 0.09946085512638092, disc_loss: 0.0016790135996416211
INFO:root:Generator loss: 0.05697140197983934, Discriminator loss: 0.003935662437708866
INFO:root:Epoch 29, Step 500, loss: 0.08036457747220993, disc_loss: 0.00181618332862854
INFO:root:Epoch 29, Step 1000, loss: 0.08410156518220901, disc_loss: 0.001275302143767476
INFO:root:Epoch 29, Step 1500, loss: 0.06379825621843338, disc_loss: 0.0008126177708618343
INFO:root:Epoch 29, Step 2000, loss: 0.05573512613773346, disc_loss: 0.005338369868695736
INFO:root:Epoch 29, Step 2500, loss: 0.05019616335630417, disc_loss: 0.0011705142678692937
INFO:root:Generator loss: 0.05751500050853757, Discriminator loss: 0.0040660404156016424
INFO:root:Epoch 30, Step 500, loss: 0.05376867577433586, disc_loss: 0.00117009156383574
INFO:root:Epoch 30, Step 1000, loss: 0.09846365451812744, disc_loss: 0.00306608690880239
INFO:root:Epoch 30, Step 1500, loss: 0.06315276026725769, disc_loss: 0.0008335723541676998
INFO:root:Epoch 30, Step 2000, loss: 0.09315390884876251, disc_loss: 0.0018219887278974056
INFO:root:Epoch 30, Step 2500, loss: 0.1029881089925766, disc_loss: 0.0013572665629908442
INFO:root:Generator loss: 0.05685839911434546, Discriminator loss: 0.004857568502876409
INFO:root:Epoch 31, Step 500, loss: 0.08629101514816284, disc_loss: 0.00035455013858154416
INFO:root:Epoch 31, Step 1000, loss: 0.083938367664814, disc_loss: 0.0017113086069002748
INFO:root:Epoch 31, Step 1500, loss: 0.0714401826262474, disc_loss: 0.0011296052252873778
INFO:root:Epoch 31, Step 2000, loss: 0.06304112821817398, disc_loss: 0.0015342846745625138
INFO:root:Epoch 31, Step 2500, loss: 0.04653649032115936, disc_loss: 0.0009484769543632865
INFO:root:Generator loss: 0.057079349762027706, Discriminator loss: 0.0043850391602355005
INFO:root:Epoch 32, Step 500, loss: 0.056937407702207565, disc_loss: 0.001375528983771801
INFO:root:Epoch 32, Step 1000, loss: 0.057657383382320404, disc_loss: 0.0022340051364153624
INFO:root:Epoch 32, Step 1500, loss: 0.06563665717840195, disc_loss: 0.0016831581015139818
INFO:root:Epoch 32, Step 2000, loss: 0.08487944304943085, disc_loss: 0.0026076810900121927
INFO:root:Epoch 32, Step 2500, loss: 0.08937281370162964, disc_loss: 0.0010753157548606396
INFO:root:Generator loss: 0.056729333601819656, Discriminator loss: 0.004037251811116698
INFO:root:Epoch 33, Step 500, loss: 0.07655515521764755, disc_loss: 0.0014207226922735572
INFO:root:Epoch 33, Step 1000, loss: 0.08598204702138901, disc_loss: 0.0013206514995545149
INFO:root:Epoch 33, Step 1500, loss: 0.06325193494558334, disc_loss: 0.0006238833302631974
INFO:root:Epoch 33, Step 2000, loss: 0.07095114141702652, disc_loss: 0.0013164682313799858
INFO:root:Epoch 33, Step 2500, loss: 0.08121106773614883, disc_loss: 0.00047971756430342793
INFO:root:Generator loss: 0.05749568890201524, Discriminator loss: 0.004621518596932837
INFO:root:Epoch 34, Step 500, loss: 0.10008329898118973, disc_loss: 0.0004465894598979503
INFO:root:Epoch 34, Step 1000, loss: 0.056686773896217346, disc_loss: 0.0005249321693554521
INFO:root:Epoch 34, Step 1500, loss: 0.07318797707557678, disc_loss: 0.0008264807984232903
INFO:root:Epoch 34, Step 2000, loss: 0.06267644464969635, disc_loss: 0.0012368307216092944
INFO:root:Epoch 34, Step 2500, loss: 0.05671964958310127, disc_loss: 0.05815936252474785
INFO:root:Generator loss: 0.05789120074867913, Discriminator loss: 0.004178101749787247
INFO:root:Epoch 35, Step 500, loss: 0.054831575602293015, disc_loss: 0.0010935552418231964
INFO:root:Epoch 35, Step 1000, loss: 0.07581688463687897, disc_loss: 0.0032463616225868464
INFO:root:Epoch 35, Step 1500, loss: 0.07154862582683563, disc_loss: 0.0011522360146045685
INFO:root:Epoch 35, Step 2000, loss: 0.09091994911432266, disc_loss: 0.0018670448334887624
INFO:root:Epoch 35, Step 2500, loss: 0.09395437687635422, disc_loss: 0.0007727313786745071
INFO:root:Generator loss: 0.05723093444644248, Discriminator loss: 0.004214044286006523
INFO:root:Epoch 36, Step 500, loss: 0.0751689150929451, disc_loss: 0.002655328018590808
INFO:root:Epoch 36, Step 1000, loss: 0.08940586447715759, disc_loss: 0.0028241360560059547
INFO:root:Epoch 36, Step 1500, loss: 0.08226495236158371, disc_loss: 0.0005964005249552429
INFO:root:Epoch 36, Step 2000, loss: 0.09321226924657822, disc_loss: 0.0013904174556955695
INFO:root:Epoch 36, Step 2500, loss: 0.06077786907553673, disc_loss: 0.00144778355024755
INFO:root:Generator loss: 0.05685391929904813, Discriminator loss: 0.004325323634245772
INFO:root:Epoch 37, Step 500, loss: 0.05229634419083595, disc_loss: 0.0022845081984996796
INFO:root:Epoch 37, Step 1000, loss: 0.0867263600230217, disc_loss: 0.0013340337900444865
INFO:root:Epoch 37, Step 1500, loss: 0.07060778886079788, disc_loss: 0.0011254744604229927
INFO:root:Epoch 37, Step 2000, loss: 0.06545994430780411, disc_loss: 7.226406887639314e-05
INFO:root:Epoch 37, Step 2500, loss: 0.06626259535551071, disc_loss: 0.0010269558988511562
INFO:root:Generator loss: 0.05672286883068895, Discriminator loss: 0.003502178935942865
INFO:root:Epoch 38, Step 500, loss: 0.05491106957197189, disc_loss: 0.002561417408287525
INFO:root:Epoch 38, Step 1000, loss: 0.07582257688045502, disc_loss: 0.0011219168081879616
INFO:root:Epoch 38, Step 1500, loss: 0.09359344840049744, disc_loss: 0.002579492749646306
INFO:root:Epoch 38, Step 2000, loss: 0.047862134873867035, disc_loss: 0.0007389222737401724
INFO:root:Epoch 38, Step 2500, loss: 0.06772506982088089, disc_loss: 0.0008614654070697725
INFO:root:Generator loss: 0.05733635917631457, Discriminator loss: 0.00435076622635429
INFO:root:Epoch 39, Step 500, loss: 0.05945456400513649, disc_loss: 0.0008183164754882455
INFO:root:Epoch 39, Step 1000, loss: 0.07141486555337906, disc_loss: 0.0020580003038048744
INFO:root:Epoch 39, Step 1500, loss: 0.06560717523097992, disc_loss: 0.0005259314784780145
INFO:root:Epoch 39, Step 2000, loss: 0.0599815808236599, disc_loss: 0.0013601670507341623
INFO:root:Epoch 39, Step 2500, loss: 0.056402310729026794, disc_loss: 0.0008746705134399235
INFO:root:Generator loss: 0.056215531382601235, Discriminator loss: 0.003920179965523822
INFO:root:Epoch 40, Step 500, loss: 0.0625813752412796, disc_loss: 0.001525794854387641
INFO:root:Epoch 40, Step 1000, loss: 0.069739930331707, disc_loss: 0.0029925997368991375
INFO:root:Epoch 40, Step 1500, loss: 0.06290390342473984, disc_loss: 0.0009289716253988445
INFO:root:Epoch 40, Step 2000, loss: 0.04725237935781479, disc_loss: 0.0038355428259819746
INFO:root:Epoch 40, Step 2500, loss: 0.10308806598186493, disc_loss: 0.0007996350177563727
INFO:root:Generator loss: 0.055763543357403536, Discriminator loss: 0.00473100360334869
INFO:root:Epoch 41, Step 500, loss: 0.06194192171096802, disc_loss: 0.0013559822691604495
INFO:root:Epoch 41, Step 1000, loss: 0.06235095486044884, disc_loss: 0.0013597896322607994
INFO:root:Epoch 41, Step 1500, loss: 0.08103148639202118, disc_loss: 0.001762984087690711
INFO:root:Epoch 41, Step 2000, loss: 0.05545410141348839, disc_loss: 0.0011309031397104263
INFO:root:Epoch 41, Step 2500, loss: 0.07615933567285538, disc_loss: 0.002402649959549308
INFO:root:Generator loss: 0.05717485036375453, Discriminator loss: 0.004708863168417841
INFO:root:Epoch 42, Step 500, loss: 0.06979230791330338, disc_loss: 0.0012329552555456758
INFO:root:Epoch 42, Step 1000, loss: 0.058289725333452225, disc_loss: 0.0006983319181017578
INFO:root:Epoch 42, Step 1500, loss: 0.07959557324647903, disc_loss: 0.0017395312897861004
INFO:root:Epoch 42, Step 2000, loss: 0.10258404165506363, disc_loss: 0.0014910864410921931
INFO:root:Epoch 42, Step 2500, loss: 0.040548015385866165, disc_loss: 0.0017057897057384253
INFO:root:Generator loss: 0.056838626394645105, Discriminator loss: 0.005500088343625658
INFO:root:Epoch 43, Step 500, loss: 0.06674187630414963, disc_loss: 0.0011735564330592752
INFO:root:Epoch 43, Step 1000, loss: 0.10310903936624527, disc_loss: 0.003072185441851616
INFO:root:Epoch 43, Step 1500, loss: 0.053080733865499496, disc_loss: 0.0009342433768324554
INFO:root:Epoch 43, Step 2000, loss: 0.07557825744152069, disc_loss: 0.004602281376719475
INFO:root:Epoch 43, Step 2500, loss: 0.0691903680562973, disc_loss: 0.0033530076034367085
INFO:root:Generator loss: 0.05671847288867513, Discriminator loss: 0.004491256010403305
INFO:root:Epoch 44, Step 500, loss: 0.07008543610572815, disc_loss: 0.0018194759031757712
INFO:root:Epoch 44, Step 1000, loss: 0.05988248810172081, disc_loss: 0.0011200510198250413
INFO:root:Epoch 44, Step 1500, loss: 0.08773816376924515, disc_loss: 0.0012394291115924716
INFO:root:Epoch 44, Step 2000, loss: 0.07643090188503265, disc_loss: 0.001213931362144649
INFO:root:Epoch 44, Step 2500, loss: 0.07629512250423431, disc_loss: 0.0006610160926356912
INFO:root:Generator loss: 0.05656162449565617, Discriminator loss: 0.003180267256715683
INFO:root:Epoch 45, Step 500, loss: 0.05791060999035835, disc_loss: 0.0009231168078258634
INFO:root:Epoch 45, Step 1000, loss: 0.06183471530675888, disc_loss: 0.0020496805664151907
INFO:root:Epoch 45, Step 1500, loss: 0.07695623487234116, disc_loss: 0.0012226662365719676
INFO:root:Epoch 45, Step 2000, loss: 0.10171087831258774, disc_loss: 0.004032885190099478
INFO:root:Epoch 45, Step 2500, loss: 0.10405146330595016, disc_loss: 0.0006224465323612094
INFO:root:Generator loss: 0.05710308996110576, Discriminator loss: 0.006055929171743982
INFO:root:Epoch 46, Step 500, loss: 0.08919573575258255, disc_loss: 0.0010434385621920228
INFO:root:Epoch 46, Step 1000, loss: 0.040715306997299194, disc_loss: 0.002078397199511528
INFO:root:Epoch 46, Step 1500, loss: 0.07055211067199707, disc_loss: 0.0011394984321668744
INFO:root:Epoch 46, Step 2000, loss: 0.0732177123427391, disc_loss: 0.0005147946649231017
INFO:root:Epoch 46, Step 2500, loss: 0.05483756214380264, disc_loss: 0.0004112280730623752
INFO:root:Generator loss: 0.057239413948603046, Discriminator loss: 0.004536685380099318
INFO:root:Epoch 47, Step 500, loss: 0.08909726142883301, disc_loss: 0.014064429327845573
INFO:root:Epoch 47, Step 1000, loss: 0.07524335384368896, disc_loss: 0.0003897806163877249
INFO:root:Epoch 47, Step 1500, loss: 0.06060535088181496, disc_loss: 0.00015396910021081567
INFO:root:Epoch 47, Step 2000, loss: 0.0896846204996109, disc_loss: 0.001823695725761354
INFO:root:Epoch 47, Step 2500, loss: 0.08876094967126846, disc_loss: 0.00045277722529135644
INFO:root:Generator loss: 0.05637026167682652, Discriminator loss: 0.003909279049252096
INFO:root:Epoch 48, Step 500, loss: 0.04895953834056854, disc_loss: 0.0022687078453600407
INFO:root:Epoch 48, Step 1000, loss: 0.061605729162693024, disc_loss: 0.0008926999871619046
INFO:root:Epoch 48, Step 1500, loss: 0.08490318804979324, disc_loss: 0.050627902150154114
INFO:root:Epoch 48, Step 2000, loss: 0.07021968066692352, disc_loss: 0.0018788048764690757
INFO:root:Epoch 48, Step 2500, loss: 0.06784085929393768, disc_loss: 0.0005965933087281883
INFO:root:Generator loss: 0.05574769014944729, Discriminator loss: 0.004357616430328544
INFO:root:Epoch 49, Step 500, loss: 0.0844813659787178, disc_loss: 0.0010956767946481705
INFO:root:Epoch 49, Step 1000, loss: 0.06554991006851196, disc_loss: 0.000693436770234257
INFO:root:Epoch 49, Step 1500, loss: 0.06750095635652542, disc_loss: 0.00027760659577324986
INFO:root:Epoch 49, Step 2000, loss: 0.08693937212228775, disc_loss: 0.0019862698391079903
INFO:root:Epoch 49, Step 2500, loss: 0.06747037172317505, disc_loss: 0.00016455996956210583
INFO:root:Generator loss: 0.05625908443867003, Discriminator loss: 0.004037537953019673
INFO:root:Epoch 50, Step 500, loss: 0.08428975194692612, disc_loss: 0.0034135086461901665
INFO:root:Epoch 50, Step 1000, loss: 0.07070793211460114, disc_loss: 0.0004292188386898488
INFO:root:Epoch 50, Step 1500, loss: 0.055500578135252, disc_loss: 0.0007230055052787066
INFO:root:Epoch 50, Step 2000, loss: 0.06954862922430038, disc_loss: 0.000303491426166147
INFO:root:Epoch 50, Step 2500, loss: 0.0904846116900444, disc_loss: 0.0028916301671415567
INFO:root:Generator loss: 0.05656072569703593, Discriminator loss: 0.005402216533378662
INFO:root:Epoch 51, Step 500, loss: 0.13698159158229828, disc_loss: 0.0006543039926327765
INFO:root:Epoch 51, Step 1000, loss: 0.046117331832647324, disc_loss: 0.0003706526185851544
INFO:root:Epoch 51, Step 1500, loss: 0.06343231350183487, disc_loss: 5.2783092542085797e-05
INFO:root:Epoch 51, Step 2000, loss: 0.09569181501865387, disc_loss: 0.001003200188279152
INFO:root:Epoch 51, Step 2500, loss: 0.061209194362163544, disc_loss: 0.0005420197849161923
INFO:root:Generator loss: 0.05641240880652828, Discriminator loss: 0.004217182571046932
INFO:root:Epoch 52, Step 500, loss: 0.05075540766119957, disc_loss: 0.0009896625997498631
INFO:root:Epoch 52, Step 1000, loss: 0.06435218453407288, disc_loss: 0.0010384581983089447
INFO:root:Epoch 52, Step 1500, loss: 0.07390189170837402, disc_loss: 0.003443030407652259
INFO:root:Epoch 52, Step 2000, loss: 0.08416915684938431, disc_loss: 0.00046997726894915104
INFO:root:Epoch 52, Step 2500, loss: 0.06219618767499924, disc_loss: 0.0016301277792081237
INFO:root:Generator loss: 0.05668417317340675, Discriminator loss: 0.004198875761838422
INFO:root:Epoch 53, Step 500, loss: 0.08889388293027878, disc_loss: 0.002101027173921466
INFO:root:Epoch 53, Step 1000, loss: 0.1029309332370758, disc_loss: 0.003981864079833031
INFO:root:Epoch 53, Step 1500, loss: 0.08297201991081238, disc_loss: 0.0005388588178902864
INFO:root:Epoch 53, Step 2000, loss: 0.06845375895500183, disc_loss: 0.0006309348973445594
INFO:root:Epoch 53, Step 2500, loss: 0.08395140618085861, disc_loss: 0.0010036894818767905
INFO:root:Generator loss: 0.05656955619930353, Discriminator loss: 0.0048586555818226915
INFO:root:Epoch 54, Step 500, loss: 0.08178799599409103, disc_loss: 0.00043749509495683014
INFO:root:Epoch 54, Step 1000, loss: 0.0840129628777504, disc_loss: 0.0010134481126442552
INFO:root:Epoch 54, Step 1500, loss: 0.08727501332759857, disc_loss: 0.00047579361125826836
INFO:root:Epoch 54, Step 2000, loss: 0.05088311806321144, disc_loss: 0.0003529624955262989
INFO:root:Epoch 54, Step 2500, loss: 0.07873211055994034, disc_loss: 0.00038665597094222903
INFO:root:Generator loss: 0.05627268690218046, Discriminator loss: 0.004406627029152553
INFO:root:Epoch 55, Step 500, loss: 0.05507051572203636, disc_loss: 0.005183336324989796
INFO:root:Epoch 55, Step 1000, loss: 0.06789188086986542, disc_loss: 0.00019709767366293818
INFO:root:Epoch 55, Step 1500, loss: 0.06391659379005432, disc_loss: 0.0007532972958870232
INFO:root:Epoch 55, Step 2000, loss: 0.047019898891448975, disc_loss: 0.0005374279571697116
INFO:root:Epoch 55, Step 2500, loss: 0.08593488484621048, disc_loss: 0.0008742962381802499
INFO:root:Generator loss: 0.0560285118182452, Discriminator loss: 0.004831194736555702
INFO:root:Epoch 56, Step 500, loss: 0.06734950840473175, disc_loss: 0.000279007013887167
INFO:root:Epoch 56, Step 1000, loss: 0.09986555576324463, disc_loss: 0.0014408589340746403
INFO:root:Epoch 56, Step 1500, loss: 0.0776502937078476, disc_loss: 0.0006958802114240825
INFO:root:Epoch 56, Step 2000, loss: 0.0953034833073616, disc_loss: 0.00031419427250511944
INFO:root:Epoch 56, Step 2500, loss: 0.04411564767360687, disc_loss: 0.0022977618500590324
INFO:root:Generator loss: 0.05617274787928005, Discriminator loss: 0.005908752329886941
INFO:root:Epoch 57, Step 500, loss: 0.06664083153009415, disc_loss: 0.0008286831434816122
INFO:root:Epoch 57, Step 1000, loss: 0.08869306743144989, disc_loss: 0.00017637932614888996
INFO:root:Epoch 57, Step 1500, loss: 0.12747439742088318, disc_loss: 0.0008649026276543736
INFO:root:Epoch 57, Step 2000, loss: 0.08469129353761673, disc_loss: 0.001461848965846002
INFO:root:Epoch 57, Step 2500, loss: 0.08746517449617386, disc_loss: 0.0003413125523366034
INFO:root:Generator loss: 0.05642671815291481, Discriminator loss: 0.003630681674824974
INFO:root:Epoch 58, Step 500, loss: 0.069893978536129, disc_loss: 0.0006308610900305212
INFO:root:Epoch 58, Step 1000, loss: 0.07054699957370758, disc_loss: 0.0009745495044626296
INFO:root:Epoch 58, Step 1500, loss: 0.07065755128860474, disc_loss: 0.0014457587385550141
INFO:root:Epoch 58, Step 2000, loss: 0.09841804206371307, disc_loss: 0.0011754394508898258
INFO:root:Epoch 58, Step 2500, loss: 0.07007145881652832, disc_loss: 0.0009704254916869104
INFO:root:Generator loss: 0.05684903284842239, Discriminator loss: 0.004762952081752868
INFO:root:Epoch 59, Step 500, loss: 0.047717053443193436, disc_loss: 0.0015287387650460005
INFO:root:Epoch 59, Step 1000, loss: 0.1217607706785202, disc_loss: 0.0010343882022425532
INFO:root:Epoch 59, Step 1500, loss: 0.07266448438167572, disc_loss: 0.0002804030664265156
INFO:root:Epoch 59, Step 2000, loss: 0.1106356531381607, disc_loss: 0.0013393991393968463
INFO:root:Epoch 59, Step 2500, loss: 0.08187257498502731, disc_loss: 0.0013885837979614735
INFO:root:Generator loss: 0.05641391760335096, Discriminator loss: 0.005259976401698755
Training finished.
Starting batch evaluation...
Evaluating: CMGAN_epoch_0_0.082
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  2.8114450119652794 csig:  4.313488471881281 cbak:  3.4733157672034833 covl:  3.619558575270314 ssnr:  8.420739228869277 stoi:  0.9385982615195235


Evaluating: CMGAN_epoch_10_0.063
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.29098763295169 csig:  4.530027255257562 cbak:  3.789562289386638 covl:  3.997683524135018 ssnr:  9.630127064949821 stoi:  0.9501190442530788


Evaluating: CMGAN_epoch_1_0.076
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  2.856269124642159 csig:  4.341162557913246 cbak:  3.528919367975354 covl:  3.6554488556937033 ssnr:  8.896345625604166 stoi:  0.9438154679711767


Evaluating: CMGAN_epoch_11_0.061
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.228982684247702 csig:  4.5096145797071845 cbak:  3.795470859776173 covl:  3.953185174210027 ssnr:  10.200985868693333 stoi:  0.9531078590592161


Evaluating: CMGAN_epoch_12_0.059
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.239756253593176 csig:  4.542650450313508 cbak:  3.817072195999147 covl:  3.9751408507715973 ssnr:  10.45940892534664 stoi:  0.9538489271849887


Evaluating: CMGAN_epoch_13_0.059
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.2429733393551077 csig:  4.542373078396083 cbak:  3.810399790210619 covl:  3.976548561744582 ssnr:  10.313513007640067 stoi:  0.9542988840898953


Evaluating: CMGAN_epoch_14_0.060
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.2928417973148014 csig:  4.531546821993318 cbak:  3.8263391650355447 covl:  3.999692962297957 ssnr:  10.19965991246564 stoi:  0.9545305815435844


Evaluating: CMGAN_epoch_15_0.058
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.3091436147111133 csig:  4.53559104204154 cbak:  3.8545051595364273 covl:  4.012556018033878 ssnr:  10.519580023788702 stoi:  0.9548609906727348


Evaluating: CMGAN_epoch_16_0.059
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.1951614211774566 csig:  4.545663748235213 cbak:  3.7942663888240675 covl:  3.949997589235176 ssnr:  10.424921837177791 stoi:  0.9554153230488793


Evaluating: CMGAN_epoch_17_0.058
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.3004167702012848 csig:  4.564145764036715 cbak:  3.8493552031843783 covl:  4.021518068678139 ssnr:  10.491549577052313 stoi:  0.955006501698527


Evaluating: CMGAN_epoch_18_0.058
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.3299188922041827 csig:  4.579667049367159 cbak:  3.868940366573374 covl:  4.043242483789366 ssnr:  10.592023625966467 stoi:  0.9550065848926294


Evaluating: CMGAN_epoch_19_0.059
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.3298261535977853 csig:  4.55069018653029 cbak:  3.834559116881848 covl:  4.030298407008053 ssnr:  10.048075044805929 stoi:  0.9530134731722779


Evaluating: CMGAN_epoch_20_0.059
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.3492418494328713 csig:  4.579864989010926 cbak:  3.8391606025377754 covl:  4.055915053881826 ssnr:  9.95528108519453 stoi:  0.9549357938500455


Evaluating: CMGAN_epoch_2_0.076
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  2.940748992764834 csig:  4.409312293123513 cbak:  3.566579886176314 covl:  3.7395693619818977 ssnr:  8.845449626503266 stoi:  0.9461576612180678


Evaluating: CMGAN_epoch_21_0.057
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.367201399166607 csig:  4.569776509246425 cbak:  3.874380689529314 covl:  4.06162917175932 ssnr:  10.414869064612105 stoi:  0.9547357463604685


Evaluating: CMGAN_epoch_22_0.058
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.2637079003655796 csig:  4.568387419266741 cbak:  3.8150626974918707 covl:  3.9973653914341365 ssnr:  10.239362488361593 stoi:  0.955463319367899


Evaluating: CMGAN_epoch_23_0.060
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.255568738701274 csig:  4.502879128378512 cbak:  3.811014878727091 covl:  3.9597828184228527 ssnr:  10.242228055532618 stoi:  0.9553915054428612


Evaluating: CMGAN_epoch_24_0.057
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.333432064907065 csig:  4.5628667245382015 cbak:  3.8594305320769777 covl:  4.037882891775806 ssnr:  10.394685547392644 stoi:  0.9562832115832616


Evaluating: CMGAN_epoch_25_0.057
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.3696504952838118 csig:  4.612651200044632 cbak:  3.878821204396079 covl:  4.08823343249546 ssnr:  10.45036462225749 stoi:  0.9555181843066026


Evaluating: CMGAN_epoch_26_0.057
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.345609064385729 csig:  4.588313747802016 cbak:  3.875612472507653 covl:  4.060787781051812 ssnr:  10.562856198081539 stoi:  0.9548624872438886


Evaluating: CMGAN_epoch_27_0.057
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.359133492685059 csig:  4.594960985712415 cbak:  3.8704943137873142 covl:  4.073790576831678 ssnr:  10.381834048337478 stoi:  0.9553995072799042


Evaluating: CMGAN_epoch_28_0.056
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.3690902354937156 csig:  4.602699386337493 cbak:  3.8915821766939613 covl:  4.084618570085019 ssnr:  10.629220363985175 stoi:  0.9555506445867463


Evaluating: CMGAN_epoch_29_0.057
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.343696300699873 csig:  4.591232407301219 cbak:  3.876977436614564 covl:  4.061760337939878 ssnr:  10.613391103851663 stoi:  0.9559600060900034


Evaluating: CMGAN_epoch_30_0.056
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.373530538481416 csig:  4.5960270170512 cbak:  3.8903791748695715 covl:  4.082976529970963 ssnr:  10.591713925012849 stoi:  0.9551614093596623


Evaluating: CMGAN_epoch_3_0.071
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  2.9703219409995865 csig:  4.443077190371736 cbak:  3.5927918855284293 covl:  3.7761720469403994 ssnr:  9.01622018948274 stoi:  0.9471842185359144


Evaluating: CMGAN_epoch_31_0.057
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.390851000703654 csig:  4.602015623884375 cbak:  3.891585645154471 covl:  4.091482110251405 ssnr:  10.48774687137649 stoi:  0.9558396137328087


Evaluating: CMGAN_epoch_32_0.056
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.3914685955325377 csig:  4.598066190740874 cbak:  3.9036506380292466 covl:  4.091221164281141 ssnr:  10.665756775404232 stoi:  0.9552990608917747


Evaluating: CMGAN_epoch_33_0.057
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.356470398532534 csig:  4.596791528684226 cbak:  3.866896081232276 covl:  4.073716203682453 ssnr:  10.35572233656377 stoi:  0.9564411360866395


Evaluating: CMGAN_epoch_34_0.057
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.350106509999164 csig:  4.590061885483535 cbak:  3.8677834390370984 covl:  4.065717061204892 ssnr:  10.416511135775853 stoi:  0.9558823885360579


Evaluating: CMGAN_epoch_35_0.057
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.317629180052905 csig:  4.592248646910262 cbak:  3.8740347759856393 covl:  4.044240069841114 ssnr:  10.749932560880978 stoi:  0.9560404089146146


Evaluating: CMGAN_epoch_36_0.056
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.3728121997951304 csig:  4.606228968388422 cbak:  3.89743137765516 covl:  4.088635545389324 ssnr:  10.699807820502162 stoi:  0.9559608853820913


Evaluating: CMGAN_epoch_37_0.056
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.386354111207342 csig:  4.612248092005061 cbak:  3.892617076246679 covl:  4.0989413465491324 ssnr:  10.531938097281818 stoi:  0.9560658633899713


Evaluating: CMGAN_epoch_38_0.057
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.327711338267743 csig:  4.5840207513362055 cbak:  3.869748114043111 covl:  4.052615553024845 ssnr:  10.609003431049642 stoi:  0.9560574193079172


Evaluating: CMGAN_epoch_39_0.056
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.3983788365877947 csig:  4.605855579128803 cbak:  3.9080304485439528 covl:  4.1051444789028855 ssnr:  10.680842279793666 stoi:  0.955655648073602


Evaluating: CMGAN_epoch_40_0.055
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.373960508738907 csig:  4.6042820647996425 cbak:  3.9005936578828058 covl:  4.0894630752899666 ssnr:  10.745767725993773 stoi:  0.9562815718833725


Evaluating: CMGAN_epoch_4_0.072
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.0636772850474108 csig:  4.469796790198921 cbak:  3.5835506133767585 covl:  3.8422256550269935 ssnr:  8.262879128175227 stoi:  0.9483474512337529


Evaluating: CMGAN_epoch_41_0.057
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.371589843943281 csig:  4.6058891360879715 cbak:  3.888599737040913 covl:  4.088068069007717 ssnr:  10.576068665787304 stoi:  0.9557149909975569


Evaluating: CMGAN_epoch_42_0.056
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.376880053900978 csig:  4.61670938266746 cbak:  3.8945921839582 covl:  4.100292382925177 ssnr:  10.633222943674427 stoi:  0.9561772995601897


Evaluating: CMGAN_epoch_43_0.056
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.336878219131127 csig:  4.604024985802105 cbak:  3.8810618854639327 covl:  4.065507923928188 ssnr:  10.714395935572044 stoi:  0.9560442415429083


Evaluating: CMGAN_epoch_44_0.056
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.344036039941519 csig:  4.600688521230526 cbak:  3.8865087137457994 covl:  4.069281848550842 ssnr:  10.753247119317413 stoi:  0.9567791015356447


Evaluating: CMGAN_epoch_45_0.057
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.329561349547025 csig:  4.579525297485885 cbak:  3.862721622176698 covl:  4.047943094915155 ssnr:  10.496476741781374 stoi:  0.9567272078270305


Evaluating: CMGAN_epoch_46_0.057
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.368052942226234 csig:  4.602749412392982 cbak:  3.87849519196371 covl:  4.082873718285738 ssnr:  10.447965656431373 stoi:  0.9559152743850685


Evaluating: CMGAN_epoch_47_0.056
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.3707344759147144 csig:  4.594385834271602 cbak:  3.895426284801667 covl:  4.079340777854236 ssnr:  10.69510460968737 stoi:  0.9562500306650313


Evaluating: CMGAN_epoch_48_0.055
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.38115628975109 csig:  4.604954004904341 cbak:  3.9044574117761117 covl:  4.092247141884271 ssnr:  10.75612282677561 stoi:  0.9559809096052224


Evaluating: CMGAN_epoch_49_0.056
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.377870493577522 csig:  4.613489397627603 cbak:  3.901395917489927 covl:  4.096379220824702 ssnr:  10.728526326726735 stoi:  0.9560327476028768


Evaluating: CMGAN_epoch_50_0.056
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.3777860709764425 csig:  4.614045091094787 cbak:  3.8970513745251787 covl:  4.097997607325998 ssnr:  10.659486078295322 stoi:  0.9557610086964672


Evaluating: CMGAN_epoch_5_0.069
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.1544004153568768 csig:  4.456202728160728 cbak:  3.6663755212287317 covl:  3.8773876890760315 ssnr:  8.74827535912972 stoi:  0.9499133124036859


Evaluating: CMGAN_epoch_51_0.056
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.361422580133364 csig:  4.594257061644536 cbak:  3.890980993146979 covl:  4.076086498329677 ssnr:  10.68837321472314 stoi:  0.9565586806067287


Evaluating: CMGAN_epoch_52_0.056
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.3779725258790174 csig:  4.599792892805184 cbak:  3.897160915040349 covl:  4.088602384777445 ssnr:  10.662371383721226 stoi:  0.9563583930770528


Evaluating: CMGAN_epoch_53_0.056
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.352325105262034 csig:  4.593016008153005 cbak:  3.8814133715530277 covl:  4.069000248974366 ssnr:  10.608422260662268 stoi:  0.9567581626688436


Evaluating: CMGAN_epoch_54_0.056
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.387124090808109 csig:  4.610178609771553 cbak:  3.9098323696906467 covl:  4.098861050463388 ssnr:  10.794159823635502 stoi:  0.9563563732275575


Evaluating: CMGAN_epoch_55_0.056
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.3793947348027555 csig:  4.604910923968376 cbak:  3.9039424328392944 covl:  4.0918570263579 ssnr:  10.763272159299348 stoi:  0.9563141860686238


Evaluating: CMGAN_epoch_56_0.056
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.3743366837790867 csig:  4.6011514457583385 cbak:  3.896824602734169 covl:  4.086563400441151 ssnr:  10.679621422850953 stoi:  0.9561117080050373


Evaluating: CMGAN_epoch_57_0.056
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.3814133977137724 csig:  4.6168250586852775 cbak:  3.8973671249075528 covl:  4.1007191670664005 ssnr:  10.637616770637157 stoi:  0.9566627247519899


Evaluating: CMGAN_epoch_58_0.056
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.3653735069973956 csig:  4.612508327757427 cbak:  3.8939519283055932 covl:  4.08720421573492 ssnr:  10.710633097801308 stoi:  0.9565268768315616


Evaluating: CMGAN_epoch_59_0.056
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.3775437250010016 csig:  4.616779353896482 cbak:  3.8931496908853416 covl:  4.098907871973816 ssnr:  10.605267652115623 stoi:  0.9562679121407041


Evaluating: CMGAN_epoch_6_0.064
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.1401241820992776 csig:  4.473669573959843 cbak:  3.7428062911682014 covl:  3.8795817008324227 ssnr:  10.057658882314174 stoi:  0.9503395214782523


Evaluating: CMGAN_epoch_7_0.066
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.207963853495792 csig:  4.529724815948296 cbak:  3.763130832824907 covl:  3.953757456754052 ssnr:  9.864259783951297 stoi:  0.9502651072792134


Evaluating: CMGAN_epoch_8_0.064
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.1899803878323545 csig:  4.479082988991612 cbak:  3.73894908218975 covl:  3.905923199459846 ssnr:  9.641237930929986 stoi:  0.9528081317693665


Evaluating: CMGAN_epoch_9_0.065
evaluation.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict((torch.load(model_path)))
pesq:  3.16095316930882 csig:  4.471965128456365 cbak:  3.737957967306526 covl:  3.8936605049206983 ssnr:  9.804159427593433 stoi:  0.952561960455195


